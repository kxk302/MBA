{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MBA",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kxk302/MBA/blob/main/MBA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JDt_5RMdjL7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0bkibi4dy-V"
      },
      "source": [
        "!ls '/content/gdrive/MyDrive/Colab Notebooks/MBA_files'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8BTJgr3VIiP"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from mlxtend.frequent_patterns import apriori\n",
        "from mlxtend.frequent_patterns import association_rules\n",
        "\n",
        "funclass_dict = {\"1\":\"Silent\", \"2\":\"Nonsense\", \"3\":\"Misense\", \"4\":\"None\"}\n",
        "af_dict = {\"1\":\"AF < 0.20\", \"2\": \"0.80 > AF >= 0.20\", \"3\":\"AF >= 0.80\"}\n",
        "\n",
        "def to_str_each(items):\n",
        "  items_str = \"\"\n",
        "  for item in items:\n",
        "    item_str = \"(\" + item[0:-2:1] + \", \" + funclass_dict.get(item[-2:-1:1]) + \", \" + af_dict.get(item[-1]) + \") & \"\n",
        "    items_str += item_str \n",
        "  return items_str[:-3]\n",
        "\n",
        "# Convert series of antecedents/consequents to human readable \n",
        "def to_str(ser):    \n",
        "  # Cast to string\n",
        "  ser = ser.astype(str)  \n",
        "\n",
        "  # Get rid of unnecessary characters prior to list of (position + funclass + af) numbers\n",
        "  ser = ser.str.slice(12,-3,1)  \n",
        "\n",
        "  # Get rid of single quotes\n",
        "  ser = ser.replace('\\'', '', regex=True)\n",
        "\n",
        "  ser = ser.str.split(pat=\",\")\n",
        "\n",
        "  return ser.apply(to_str_each)\n",
        "\n",
        "def add_readable_rules(df_in):\n",
        "  # Empty data frame\n",
        "  if df_in is None or df_in.shape[0] == 0:\n",
        "    return df_in\n",
        "\n",
        "  df = df_in.copy()\n",
        "\n",
        "  antecedents_str = to_str(df['antecedents'])\n",
        "  consequents_str = to_str(df['consequents'])\n",
        "\n",
        "  readable_rule = \"(\" + antecedents_str + \") => (\" + consequents_str + \")\"\n",
        "  df.insert(2,'readable_rule', readable_rule)\n",
        "\n",
        "  return df\n",
        "\n",
        "def filter_on_position_probability(df_in, start_prob=None, end_prob=None):\n",
        "    if df_in is None or df_in.shape[0] == 0:\n",
        "      return df_in\n",
        "\n",
        "    if start_prob is None and end_prob is None:\n",
        "      return df_in\n",
        "\n",
        "    # Only consider rows where probability of Position being present in the Samples is >= start_prob and <= end_prob\n",
        "    num_samples = df_in['Sample'].nunique()\n",
        "    print(\"num_samples: {}\".format(num_samples))\n",
        "\n",
        "    num_positions = df_in['POS'].nunique()\n",
        "    print(\"num_positions: {}\".format(num_positions))\n",
        "   \n",
        "    df = df_in[['Sample', 'POS']].groupby(['POS']).agg(position_prob=('Sample', 'count'))\n",
        "    # Normalize position_prob by dividing it by num_samples. This makes it a proability between 0.0 and 1.0\n",
        "    df['position_prob'] = df['position_prob'] / num_samples   \n",
        "\n",
        "    df_sorted = df.sort_values(by = 'position_prob')\n",
        "    print(df_sorted.head())\n",
        "    print(df_sorted.tail())\n",
        "  \n",
        "    if start_prob is not None:\n",
        "      df = df[df.position_prob >= start_prob]\n",
        "\n",
        "    if end_prob is not None:\n",
        "      df = df[df.position_prob <= end_prob]\n",
        "    \n",
        "    ret_val = pd.merge(df_in, df, on='POS')\n",
        "\n",
        "    df_sorted = ret_val.sort_values(by = 'position_prob')[['POS', 'position_prob']]\n",
        "    print(df_sorted.head())\n",
        "    print(df_sorted.tail())\n",
        "\n",
        "    return ret_val\n",
        "\n",
        "def get_position_probability(df_in, position=None):\n",
        "    if df_in is None or df_in.shape[0] == 0:\n",
        "      return df_in\n",
        "\n",
        "    if position is None:\n",
        "      return df_in\n",
        "\n",
        "    num_samples = df_in['Sample'].nunique()\n",
        "    print(\"num_samples: {}\".format(num_samples))\n",
        "\n",
        "    num_positions = df_in['POS'].nunique()\n",
        "    print(\"num_positions: {}\".format(num_positions))\n",
        "   \n",
        "    df = df_in[['Sample', 'POS']].groupby(['POS']).agg(position_prob=('Sample', 'count'))\n",
        "    # Normalize position_prob by dividing it by num_samples. This makes it a proability between 0.0 and 1.0\n",
        "    df['position_prob'] = df['position_prob'] / num_samples\n",
        "\n",
        "    try:      \n",
        "      return df.loc[position,'position_prob']\n",
        "    except KeyError:\n",
        "      return -1\n",
        "\n",
        "def filter_on_column_values(df_in, start_pos=None, end_pos=None, start_af=None, end_af=None, funclass=[]):  \n",
        "  if df_in is None or df_in.shape[0] == 0:\n",
        "    return df_in\n",
        "\n",
        "  df = df_in.copy()\n",
        "\n",
        "  # Replace \".\" with \"NONE\" in FUNCLASS column. They both represent \"Non-coding\" variant\n",
        "  df[\"FUNCLASS\"] = df[\"FUNCLASS\"].replace('.', 'NONE')\n",
        "\n",
        "  # Filter in_file rows\n",
        "  if start_pos is not None:\n",
        "    df = df[df.POS >= start_pos]    \n",
        "  if end_pos is not None:\n",
        "    df = df[df.POS <= end_pos]    \n",
        "  if start_af is not None:\n",
        "    df = df[df.AF >= start_af]    \n",
        "  if end_af is not None:\n",
        "    df = df[df.AF <= end_af]    \n",
        "  if len(funclass) > 0:    \n",
        "    df = df[df.FUNCLASS.isin(funclass)]\n",
        "\n",
        "  return df\n",
        "\n",
        "# Create a single integer representing a variant at a specific position with a specific allele frequency\n",
        "# Pivot the data so we have all sample variants on a single line\n",
        "#\n",
        "# Extract rows from in_file where\n",
        "# \n",
        "#    POS >= start_pos & POS <= end_pos\n",
        "#        If start_pos = None: POS <= end_pos\n",
        "#        If end_pos = None: POS >= start_pos \n",
        "#\n",
        "#    AF >= start_af & AF <= end_af\n",
        "#        If start_af = None: AF <= end_af\n",
        "#        If end_af = None: AF >= start_af\n",
        "#\n",
        "#    FUNCLASS in funclass (funclass is a list)      \n",
        "#\n",
        "def preprocess_input_file(df_in):\n",
        "  if df_in is None or df_in.shape[0] == 0:\n",
        "    return df_in\n",
        "  \n",
        "  df = df_in.copy()\n",
        "\n",
        "  # Bucket values in FUNCLASS and AF columns. We do not bucket the values in POS column.\n",
        "\n",
        "  # Replace variants in FUNCLASS column with a distinct numeric value\n",
        "  df.loc[df.FUNCLASS == \"NONE\", \"FUNCLASS\"] = 4\n",
        "  df.loc[df.FUNCLASS == \"MISSENSE\", \"FUNCLASS\"] = 3\n",
        "  df.loc[df.FUNCLASS == \"NONSENSE\", \"FUNCLASS\"] = 2 \n",
        "  df.loc[df.FUNCLASS == \"SILENT\", \"FUNCLASS\"] = 1 \n",
        "\n",
        "  # Replace allele frequency in AF column with a distict numeric value\n",
        "  df.loc[df.AF >= 0.80, \"AF\"] = 3\n",
        "  df.loc[(df.AF >= 0.20) & (df.AF < 0.80), \"AF\"] = 2\n",
        "  df.loc[df.AF < 0.20, \"AF\"] = 1\n",
        "\n",
        "  # Convert AF values to integer\n",
        "  df = df.astype({\"AF\": int}) \n",
        "\n",
        "  # Create a new column called 'Label', which is a string concatentation of POS, FUNCLASS, and AF values. \n",
        "  # The idea is to represent each variant + allele frequency + position as a single integer, to be used in MBA \n",
        "  df[\"Label\"] = df[\"POS\"].astype(str) + df[\"FUNCLASS\"].astype(str) + df[\"AF\"].astype(str)\n",
        "\n",
        "  # We do not need POS, FUNCLASS, and AF columns anymore\n",
        "  df = df[[\"Sample\", \"Label\"]]\n",
        "  \n",
        "  # Add a new column called 'Value', prepopulated with 1\n",
        "  df[\"Value\"] = 1\n",
        "\n",
        "  df = pd.pivot_table(df, index=\"Sample\", columns=\"Label\", values=\"Value\")\n",
        "\n",
        "  # Set all data frame nan (not a number) values to 0\n",
        "  df = df.fillna(0)\n",
        "\n",
        "  # Convert all data framevalues to integer\n",
        "  df = df.astype(int) \n",
        "\n",
        "  return df"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VubP1ctVUsc"
      },
      "source": [
        "def get_association_rules(in_file, min_support=0.20, min_confidence=0.80, min_lift=1.0, min_conviction=1.0, max_len=None, start_pos=None, end_pos=None, start_af=None, end_af=None, funclass=[], start_prob=None, end_prob=None):\n",
        "  # Read the input file and pick the needed columns\n",
        "  df_in = pd.read_csv(in_file, sep='\\t')[['Sample', 'POS', 'AF', 'FUNCLASS']]\n",
        "\n",
        "  # Filter on position probability\n",
        "  df_pp = filter_on_position_probability(df_in, start_prob, end_prob)\n",
        "\n",
        "  # Filter on column values\n",
        "  df_cv = filter_on_column_values(df_pp, start_pos, end_pos, start_af, end_af, funclass)\n",
        "\n",
        "  # Preprocess the data frame\n",
        "  df = preprocess_input_file(df_cv)\n",
        "\n",
        "  # Get frequent item sets, with support larger than min_support, using Apriori algorithm\n",
        "  frequent_itemsets = apriori(df, min_support=min_support, use_colnames=True, max_len=max_len)\n",
        "\n",
        "  # Get association rules, with lift larger than min_lift  \n",
        "  rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=min_lift)\n",
        "\n",
        "  # Filter association rules, keeping rules with confidence larger than min_confidence\n",
        "  rules = rules[ (rules['confidence'] >= min_confidence) & (rules['conviction'] >= min_conviction) ]\n",
        "\n",
        "  return rules"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxoxfSG9fRA3"
      },
      "source": [
        "pd.set_option('max_columns', 10, 'display.expand_frame_repr', False)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "bos_rules = get_association_rules(in_file=\"https://github.com/galaxyproject/SARS-CoV-2/raw/master/data/var/bos_by_sample.tsv.gz\", min_support=0.223, min_confidence=0.905, min_lift=2.863, min_conviction=7.0)\n",
        "num_rules = bos_rules.shape[0]\n",
        "print('Boston dataset association rules: ')\n",
        "bos_rules = add_readable_rules(bos_rules)\n",
        "print(bos_rules.head(num_rules))\n",
        "print('\\n\\n')\n",
        "# bos_rules.to_csv('/content/gdrive/MyDrive/Colab Notebooks/MBA_files/bos_0223_0905_2863_7000.csv', sep=',')\n",
        "\n",
        "uke_rules = get_association_rules(in_file=\"https://github.com/galaxyproject/SARS-CoV-2/raw/master/data/var/cog_20200917_by_sample.tsv.gz\", min_support=0.21, min_confidence=0.91, min_lift=2.5, min_conviction=7.0)\n",
        "num_rules = uke_rules.shape[0]\n",
        "print('UK early dataset association rules: ')\n",
        "uke_rules = add_readable_rules(uke_rules)\n",
        "print(uke_rules.head(num_rules))\n",
        "print('\\n\\n')\n",
        "# uke_rules.to_csv('/content/gdrive/MyDrive/Colab Notebooks/MBA_files/uke_0210_0910_2500_7000.csv', sep=',')\n",
        "\n",
        "ukl_rules = get_association_rules(in_file=\"https://github.com/galaxyproject/SARS-CoV-2/raw/master/data/var/cog_20201120_by_sample.tsv.gz\", min_support=0.203, min_confidence=0.926, min_lift=2.03, min_conviction=7.0)\n",
        "num_rules = ukl_rules.shape[0]\n",
        "print('Uk late dataset association rules: ')\n",
        "ukl_rules = add_readable_rules(ukl_rules)\n",
        "print(ukl_rules.head(num_rules))\n",
        "print('\\n\\n')\n",
        "# ukl_rules.to_csv('/content/gdrive/MyDrive/Colab Notebooks/MBA_files/ukl_0203_0926_2030_7000.csv', sep=',')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uy3YlHqEyPGK"
      },
      "source": [
        "# Milad \n",
        "pd.set_option('max_columns', 10, 'display.expand_frame_repr', False)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "bos_rules = get_association_rules(in_file=\"https://github.com/galaxyproject/SARS-CoV-2/raw/master/data/var/bos_by_sample.tsv.gz\", min_support=0.005, min_confidence=0.05, min_lift=1.00, min_conviction=1.0, max_len=2, start_pos=21563, end_pos=25384, start_af=0.19, end_af=None, funclass=[\"MISSENSE\", \"NONSENSE\"])\n",
        "num_rules = bos_rules.shape[0]\n",
        "print('Boston dataset association rules: ')\n",
        "bos_rules = add_readable_rules(bos_rules)\n",
        "print(bos_rules.head(num_rules))\n",
        "print('\\n\\n')\n",
        "# bos_rules.to_csv('/content/gdrive/MyDrive/Colab Notebooks/MBA_files/bos_0223_0905_2863_7000.csv', sep=',')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5erF_k0uKLI",
        "outputId": "4cdd0a17-cd16-4d8e-c6a1-e0ad8d402c58"
      },
      "source": [
        "# Anton\n",
        "pd.set_option('max_columns', 10, 'display.expand_frame_repr', False)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "bos_rules = get_association_rules(in_file=\"https://github.com/galaxyproject/SARS-CoV-2/raw/master/data/var/bos_by_sample.tsv.gz\", min_support=0.050, min_confidence=0.800, min_lift=1.0, min_conviction=1.0, max_len=3, start_prob=0.000, end_prob=0.40)\n",
        "num_rules = bos_rules.shape[0]\n",
        "print('Boston dataset association rules: ')\n",
        "bos_rules = add_readable_rules(bos_rules)\n",
        "print(bos_rules.head(num_rules))\n",
        "print('\\n\\n')\n"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num_samples: 639\n",
            "num_positions: 1013\n",
            "       position_prob\n",
            "POS                 \n",
            "28893       0.001565\n",
            "18546       0.001565\n",
            "28647       0.001565\n",
            "18636       0.001565\n",
            "18714       0.001565\n",
            "       position_prob\n",
            "POS                 \n",
            "25563       0.852895\n",
            "241         0.948357\n",
            "23403       0.949922\n",
            "14408       0.951487\n",
            "3037        0.951487\n",
            "        POS  position_prob\n",
            "5997   5431       0.001565\n",
            "5534  26678       0.001565\n",
            "5533  28699       0.001565\n",
            "5532  29000       0.001565\n",
            "5531  28713       0.001565\n",
            "       POS  position_prob\n",
            "1287  2416       0.392801\n",
            "1288  2416       0.392801\n",
            "1289  2416       0.392801\n",
            "1276  2416       0.392801\n",
            "1448  2416       0.392801\n",
            "Boston dataset association rules: \n",
            "           antecedents         consequents                                                                                 readable_rule  antecedent support  consequent support   support  confidence       lift  leverage  conviction\n",
            "0              (10543)           (1887713)                                    ((105, None, AF >= 0.80)) => ((18877, Silent, AF >= 0.80))            0.134796            0.145768  0.134796    1.000000   6.860215  0.115147         inf\n",
            "1            (1887713)             (10543)                                    ((18877, Silent, AF >= 0.80)) => ((105, None, AF >= 0.80))            0.145768            0.134796  0.134796    0.924731   6.860215  0.115147   11.494850\n",
            "14           (2071631)            (105933)                                ((20716, Misense, AF < 0.20)) => ((1059, Misense, AF >= 0.80))            0.057994            0.315047  0.050157    0.864865   2.745193  0.031886    5.068652\n",
            "26           (2796433)            (105933)                               ((27964, Misense, AF >= 0.80)) => ((1059, Misense, AF >= 0.80))            0.051724            0.315047  0.051724    1.000000   3.174129  0.035429         inf\n",
            "28            (389233)            (105933)                                ((3892, Misense, AF >= 0.80)) => ((1059, Misense, AF >= 0.80))            0.105016            0.315047  0.105016    1.000000   3.174129  0.071931         inf\n",
            "..                 ...                 ...                                                                                           ...                 ...                 ...       ...         ...        ...       ...         ...\n",
            "658   (2654231, 82311)           (2654531)    ((26542, Misense, AF < 0.20) & ( 823, Silent, AF < 0.20)) => ((26545, Misense, AF < 0.20))            0.061129            0.336991  0.054859    0.897436   2.663089  0.034259    6.464342\n",
            "662  (2654231, 953511)           (2654531)   ((26542, Misense, AF < 0.20) & ( 9535, Silent, AF < 0.20)) => ((26545, Misense, AF < 0.20))            0.054859            0.336991  0.053292    0.971429   2.882658  0.034805   23.205329\n",
            "664  (2654531, 953511)           (2654231)   ((26545, Misense, AF < 0.20) & ( 9535, Silent, AF < 0.20)) => ((26542, Misense, AF < 0.20))            0.054859            0.260188  0.053292    0.971429   3.733563  0.039018   25.893417\n",
            "666           (953511)  (2654231, 2654531)   ((9535, Silent, AF < 0.20)) => ((26542, Misense, AF < 0.20) & ( 26545, Misense, AF < 0.20))            0.057994            0.235110  0.053292    0.918919   3.908468  0.039657    9.433647\n",
            "669  (2903921, 576531)            (576631)  ((29039, Nonsense, AF < 0.20) & ( 5765, Misense, AF < 0.20)) => ((5766, Misense, AF < 0.20))            0.050157            0.089342  0.050157    1.000000  11.192982  0.045676         inf\n",
            "\n",
            "[136 rows x 10 columns]\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEa5yn9JFJDI",
        "outputId": "8c9f195c-cb2d-4e74-e917-12c8094c2274"
      },
      "source": [
        "# Test get_position_probability()\n",
        "\n",
        "pd.set_option('max_columns', 10, 'display.expand_frame_repr', False)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# Read the input file and pick the needed columns\n",
        "in_file = \"https://github.com/galaxyproject/SARS-CoV-2/raw/master/data/var/bos_by_sample.tsv.gz\"\n",
        "df_in = pd.read_csv(in_file, sep='\\t')[['Sample', 'POS', 'AF', 'FUNCLASS']]\n",
        "\n",
        "position=21\n",
        "position_prob = get_position_probability(df_in, position=position)\n",
        "print(\"position: {}, position_prob: {:.4f}\".format(position, position_prob))\n",
        "\n",
        "position=30\n",
        "position_prob = get_position_probability(df_in, position=position)\n",
        "print(\"position: {}, position_prob: {:.4f}\".format(position, position_prob))\n",
        "\n",
        "position=29806\n",
        "position_prob = get_position_probability(df_in, position=position)\n",
        "print(\"position: {}, position_prob: {:.4f}\".format(position, position_prob))\n"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num_samples: 639\n",
            "num_positions: 1013\n",
            "position: 21, position_prob: 0.0016\n",
            "num_samples: 639\n",
            "num_positions: 1013\n",
            "position: 30, position_prob: -1.0000\n",
            "num_samples: 639\n",
            "num_positions: 1013\n",
            "position: 29806, position_prob: 0.0078\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3ASKXDBeOsr"
      },
      "source": [
        "---\n",
        "**MBA parameter**s \n",
        "\n",
        "1.   **bos_rules**: *support*=0.223, *confidence*=0.905, *lift*=2.863, *conviction*=7.000\n",
        "2.   **uke_rules**: *support*=0.210, *confidence*=0.910, *lift*=2.500, conviction *italicized text*=7.000\n",
        "3.   **ukl_rules**: *support*=0.203, *confidence*=0.926, *lift*=2.030, *conviction*=7.000\n",
        "---\n",
        "The last digit of an entry is Allele Frequency (**AF**)\n",
        "\n",
        "*   **3**: $>=$ 0.80    \n",
        "*   **2**: $>=$ 0.20 and $<$ 0.80\n",
        "*   **1**: $<$ 0.20\n",
        "---\n",
        "The digit before the last is **Funclass**\n",
        "\n",
        "*  **4**: None\n",
        "*  **3**: Missense\n",
        "*  **2**: Nonsense\n",
        "*  **1**: Silent\n",
        "---\n",
        "**Boston association rules** (12 rules)\n",
        "\n",
        "**Antecedent** entries\n",
        "\n",
        "* 3037**13**  (5 times)\n",
        "* 14408**33** (5 times)\n",
        "* 23403**33** (5 times)\n",
        "* 26542**31** (All 12)\n",
        "\n",
        "**Consequent** entries\n",
        "\n",
        "* 3037**13** (5 times)\n",
        "* 14408**33** (5 times)\n",
        "* 23403**33** (5 times)\n",
        "* 26545**31** (All 12)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XszXqioqokmE"
      },
      "source": [
        "---\n",
        "**UK early association rules** (10 rules)\n",
        "\n",
        "**Antecedent** entries\n",
        "\n",
        "* 241**43**   (9 times)\n",
        "* 14408**33** (3 times)\n",
        "* 23403**33** (4 times)\n",
        "* 28881**32** (All 10)\n",
        "\n",
        "**Consequent** entries\n",
        "\n",
        "* 3037**12**  (All 10)\n",
        "* 14408**33** (3 times)\n",
        "* 23403**33** (3 times)\n",
        "* 28883**33** (All 10)\n",
        "\n",
        "---\n",
        "**UK late association rules** (11 rules)\n",
        "\n",
        "**Antecedent** entries\n",
        "\n",
        "* 204**42** (All 11)\n",
        "* 445**13** (All 11)\n",
        "* 6286**13** (3 times)\n",
        "* 21255**13** (All 11)\n",
        "* 23403**33** (3 times)\n",
        "* 28932**32** (1 time)\n",
        "\n",
        "**Consequent** entries\n",
        "\n",
        "* 6286**13** (5 times)\n",
        "* 22227**32** (All 11)\n",
        "* 23403**33** (5 times)\n",
        "* 27944**13** (10 times)\n",
        "* 28932**32** (9 time)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTP92PqMymr8"
      },
      "source": [
        "**Entries that show up in antecedent/consequent across samples**\n",
        "\n",
        "**Antecedent**\n",
        "\n",
        "* 14408**33** (Boston, UKE)\n",
        "* 23403**33** (Boston, UKE, UKL)\n",
        "\n",
        "**Consequent**\n",
        "\n",
        "* 3037**13** (Boston, UKE)\n",
        "* 14408**33** (Boston, UKE)\n",
        "* 23403**33** (Boston, UKE, UKL)\n",
        "\n"
      ]
    }
  ]
}