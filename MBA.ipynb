{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MBA",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kxk302/MBA/blob/main/MBA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JDt_5RMdjL7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0bkibi4dy-V"
      },
      "source": [
        "!ls '/content/gdrive/MyDrive/Colab Notebooks/MBA_files'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8BTJgr3VIiP"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from mlxtend.frequent_patterns import apriori\n",
        "from mlxtend.frequent_patterns import association_rules\n",
        "\n",
        "funclass_dict = {\"1\":\"Silent\", \"2\":\"Nonsense\", \"3\":\"Misense\", \"4\":\"None\"}\n",
        "af_dict = {\"1\":\"AF < 0.20\", \"2\": \"0.80 > AF >= 0.20\", \"3\":\"AF >= 0.80\"}\n",
        "\n",
        "def to_str_each(items):\n",
        "  items_str = \"\"\n",
        "  for item in items:\n",
        "    item_str = \"(\" + item[0:-2:1] + \", \" + funclass_dict.get(item[-2:-1:1]) + \", \" + af_dict.get(item[-1]) + \") & \"\n",
        "    items_str += item_str \n",
        "  return items_str[:-3]\n",
        "\n",
        "# Convert series of antecedents/consequents to human readable \n",
        "def to_str(ser):    \n",
        "  # Cast to string\n",
        "  ser = ser.astype(str)  \n",
        "\n",
        "  # Get rid of unnecessary characters prior to list of (position + funclass + af) numbers\n",
        "  ser = ser.str.slice(12,-3,1)  \n",
        "\n",
        "  # Get rid of single quotes\n",
        "  ser = ser.replace('\\'', '', regex=True)\n",
        "\n",
        "  ser = ser.str.split(pat=\",\")\n",
        "\n",
        "  return ser.apply(to_str_each)\n",
        "\n",
        "def add_readable_rules(df_in):\n",
        "  # Empty data frame\n",
        "  if df_in is None or df_in.shape[0] == 0:\n",
        "    return df_in\n",
        "\n",
        "  df = df_in.copy()\n",
        "\n",
        "  antecedents_str = to_str(df['antecedents'])\n",
        "  consequents_str = to_str(df['consequents'])\n",
        "\n",
        "  readable_rule = \"(\" + antecedents_str + \") => (\" + consequents_str + \")\"\n",
        "  df.insert(2,'readable_rule', readable_rule)\n",
        "\n",
        "  return df\n",
        "\n",
        "def filter_on_position_probability(df_in, start_prob=None, end_prob=None):\n",
        "    if df_in is None or df_in.shape[0] == 0:\n",
        "      return df_in\n",
        "\n",
        "    if start_prob is None and end_prob is None:\n",
        "      return df_in\n",
        "\n",
        "    # Only consider rows where probability of Position being present in the Samples is >= start_prob and <= end_prob\n",
        "    num_samples = df_in['Sample'].nunique()\n",
        "    print(\"num_samples: {}\".format(num_samples))\n",
        "\n",
        "    num_positions = df_in['POS'].nunique()\n",
        "    print(\"num_positions: {}\".format(num_positions))\n",
        "   \n",
        "    df = df_in[['Sample', 'POS']].groupby(['POS']).agg(position_prob=('Sample', 'count'))\n",
        "    # Normalize position_prob by dividing it by num_samples. This makes it a proability between 0.0 and 1.0\n",
        "    df['position_prob'] = df['position_prob'] / num_samples    \n",
        "  \n",
        "    if start_prob is not None:\n",
        "      df = df[df.position_prob >= start_prob]\n",
        "\n",
        "    if end_prob is not None:\n",
        "      df = df[df.position_prob <= end_prob]    \n",
        "    \n",
        "    return pd.merge(df_in, df, on='POS')\n",
        "\n",
        "def filter_on_column_values(df_in, start_pos=None, end_pos=None, start_af=None, end_af=None, funclass=[]):  \n",
        "  if df_in is None or df_in.shape[0] == 0:\n",
        "    return df_in\n",
        "\n",
        "  df = df_in.copy()\n",
        "\n",
        "  # Replace \".\" with \"NONE\" in FUNCLASS column. They both represent \"Non-coding\" variant\n",
        "  df[\"FUNCLASS\"] = df[\"FUNCLASS\"].replace('.', 'NONE')\n",
        "\n",
        "  # Filter in_file rows\n",
        "  if start_pos is not None:\n",
        "    df = df[df.POS >= start_pos]    \n",
        "  if end_pos is not None:\n",
        "    df = df[df.POS <= end_pos]    \n",
        "  if start_af is not None:\n",
        "    df = df[df.AF >= start_af]    \n",
        "  if end_af is not None:\n",
        "    df = df[df.AF <= end_af]    \n",
        "  if len(funclass) > 0:    \n",
        "    df = df[df.FUNCLASS.isin(funclass)]\n",
        "\n",
        "  return df\n",
        "\n",
        "# Create a single integer representing a variant at a specific position with a specific allele frequency\n",
        "# Pivot the data so we have all sample variants on a single line\n",
        "#\n",
        "# Extract rows from in_file where\n",
        "# \n",
        "#    POS >= start_pos & POS <= end_pos\n",
        "#        If start_pos = None: POS <= end_pos\n",
        "#        If end_pos = None: POS >= start_pos \n",
        "#\n",
        "#    AF >= start_af & AF <= end_af\n",
        "#        If start_af = None: AF <= end_af\n",
        "#        If end_af = None: AF >= start_af\n",
        "#\n",
        "#    FUNCLASS in funclass (funclass is a list)      \n",
        "#\n",
        "def preprocess_input_file(df_in):\n",
        "  if df_in is None or df_in.shape[0] == 0:\n",
        "    return df_in\n",
        "  \n",
        "  df = df_in.copy()\n",
        "\n",
        "  # Bucket values in FUNCLASS and AF columns. We do not bucket the values in POS column.\n",
        "\n",
        "  # Replace variants in FUNCLASS column with a distinct numeric value\n",
        "  df.loc[df.FUNCLASS == \"NONE\", \"FUNCLASS\"] = 4\n",
        "  df.loc[df.FUNCLASS == \"MISSENSE\", \"FUNCLASS\"] = 3\n",
        "  df.loc[df.FUNCLASS == \"NONSENSE\", \"FUNCLASS\"] = 2 \n",
        "  df.loc[df.FUNCLASS == \"SILENT\", \"FUNCLASS\"] = 1 \n",
        "\n",
        "  # Replace allele frequency in AF column with a distict numeric value\n",
        "  df.loc[df.AF >= 0.80, \"AF\"] = 3\n",
        "  df.loc[(df.AF >= 0.20) & (df.AF < 0.80), \"AF\"] = 2\n",
        "  df.loc[df.AF < 0.20, \"AF\"] = 1\n",
        "\n",
        "  # Convert AF values to integer\n",
        "  df = df.astype({\"AF\": int}) \n",
        "\n",
        "  # Create a new column called 'Label', which is a string concatentation of POS, FUNCLASS, and AF values. \n",
        "  # The idea is to represent each variant + allele frequency + position as a single integer, to be used in MBA \n",
        "  df[\"Label\"] = df[\"POS\"].astype(str) + df[\"FUNCLASS\"].astype(str) + df[\"AF\"].astype(str)\n",
        "\n",
        "  # We do not need POS, FUNCLASS, and AF columns anymore\n",
        "  df = df[[\"Sample\", \"Label\"]]\n",
        "  \n",
        "  # Add a new column called 'Value', prepopulated with 1\n",
        "  df[\"Value\"] = 1\n",
        "\n",
        "  df = pd.pivot_table(df, index=\"Sample\", columns=\"Label\", values=\"Value\")\n",
        "\n",
        "  # Set all data frame nan (not a number) values to 0\n",
        "  df = df.fillna(0)\n",
        "\n",
        "  # Convert all data framevalues to integer\n",
        "  df = df.astype(int) \n",
        "\n",
        "  return df"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VubP1ctVUsc"
      },
      "source": [
        "def get_association_rules(in_file, min_support=0.20, min_confidence=0.80, min_lift=1.0, min_conviction=1.0, max_len=None, start_pos=None, end_pos=None, start_af=None, end_af=None, funclass=[], start_prob=None, end_prob=None):\n",
        "  # Read the input file and pick the needed columns\n",
        "  df_in = pd.read_csv(in_file, sep='\\t')[['Sample', 'POS', 'AF', 'FUNCLASS']]\n",
        "\n",
        "  # Filter on position probability\n",
        "  df_pp = filter_on_position_probability(df_in, start_prob, end_prob)\n",
        "\n",
        "  # Filter on column values\n",
        "  df_cv = filter_on_column_values(df_pp, start_pos, end_pos, start_af, end_af, funclass)\n",
        "\n",
        "  # Preprocess the data frame\n",
        "  df = preprocess_input_file(df_cv)\n",
        "\n",
        "  # Get frequent item sets, with support larger than min_support, using Apriori algorithm\n",
        "  frequent_itemsets = apriori(df, min_support=min_support, use_colnames=True, max_len=max_len)\n",
        "\n",
        "  # Get association rules, with lift larger than min_lift  \n",
        "  rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=min_lift)\n",
        "\n",
        "  # Filter association rules, keeping rules with confidence larger than min_confidence\n",
        "  rules = rules[ (rules['confidence'] >= min_confidence) & (rules['conviction'] >= min_conviction) ]\n",
        "\n",
        "  return rules"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxoxfSG9fRA3"
      },
      "source": [
        "pd.set_option('max_columns', 10, 'display.expand_frame_repr', False)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "bos_rules = get_association_rules(in_file=\"https://github.com/galaxyproject/SARS-CoV-2/raw/master/data/var/bos_by_sample.tsv.gz\", min_support=0.223, min_confidence=0.905, min_lift=2.863, min_conviction=7.0)\n",
        "num_rules = bos_rules.shape[0]\n",
        "print('Boston dataset association rules: ')\n",
        "bos_rules = add_readable_rules(bos_rules)\n",
        "print(bos_rules.head(num_rules))\n",
        "print('\\n\\n')\n",
        "# bos_rules.to_csv('/content/gdrive/MyDrive/Colab Notebooks/MBA_files/bos_0223_0905_2863_7000.csv', sep=',')\n",
        "\n",
        "uke_rules = get_association_rules(in_file=\"https://github.com/galaxyproject/SARS-CoV-2/raw/master/data/var/cog_20200917_by_sample.tsv.gz\", min_support=0.21, min_confidence=0.91, min_lift=2.5, min_conviction=7.0)\n",
        "num_rules = uke_rules.shape[0]\n",
        "print('UK early dataset association rules: ')\n",
        "uke_rules = add_readable_rules(uke_rules)\n",
        "print(uke_rules.head(num_rules))\n",
        "print('\\n\\n')\n",
        "# uke_rules.to_csv('/content/gdrive/MyDrive/Colab Notebooks/MBA_files/uke_0210_0910_2500_7000.csv', sep=',')\n",
        "\n",
        "ukl_rules = get_association_rules(in_file=\"https://github.com/galaxyproject/SARS-CoV-2/raw/master/data/var/cog_20201120_by_sample.tsv.gz\", min_support=0.203, min_confidence=0.926, min_lift=2.03, min_conviction=7.0)\n",
        "num_rules = ukl_rules.shape[0]\n",
        "print('Uk late dataset association rules: ')\n",
        "ukl_rules = add_readable_rules(ukl_rules)\n",
        "print(ukl_rules.head(num_rules))\n",
        "print('\\n\\n')\n",
        "# ukl_rules.to_csv('/content/gdrive/MyDrive/Colab Notebooks/MBA_files/ukl_0203_0926_2030_7000.csv', sep=',')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uy3YlHqEyPGK"
      },
      "source": [
        "# Milad \n",
        "pd.set_option('max_columns', 10, 'display.expand_frame_repr', False)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "bos_rules = get_association_rules(in_file=\"https://github.com/galaxyproject/SARS-CoV-2/raw/master/data/var/bos_by_sample.tsv.gz\", min_support=0.005, min_confidence=0.05, min_lift=1.00, min_conviction=1.0, max_len=2, start_pos=21563, end_pos=25384, start_af=0.19, end_af=None, funclass=[\"MISSENSE\", \"NONSENSE\"])\n",
        "num_rules = bos_rules.shape[0]\n",
        "print('Boston dataset association rules: ')\n",
        "bos_rules = add_readable_rules(bos_rules)\n",
        "print(bos_rules.head(num_rules))\n",
        "print('\\n\\n')\n",
        "# bos_rules.to_csv('/content/gdrive/MyDrive/Colab Notebooks/MBA_files/bos_0223_0905_2863_7000.csv', sep=',')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5erF_k0uKLI",
        "outputId": "f62e9f34-3d01-4996-cc2c-3ce619e77bb5"
      },
      "source": [
        "# Anton\n",
        "pd.set_option('max_columns', 10, 'display.expand_frame_repr', False)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "bos_rules = get_association_rules(in_file=\"https://github.com/galaxyproject/SARS-CoV-2/raw/master/data/var/bos_by_sample.tsv.gz\", min_support=0.050, min_confidence=0.800, min_lift=1.0, min_conviction=1.0, start_prob=0.000, end_prob=None)\n",
        "num_rules = bos_rules.shape[0]\n",
        "print('Boston dataset association rules: ')\n",
        "bos_rules = add_readable_rules(bos_rules)\n",
        "print(bos_rules.head(num_rules))\n",
        "print('\\n\\n')\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num_samples: 639\n",
            "num_positions: 1013\n",
            "Boston dataset association rules: \n",
            "                                antecedents                                                   consequents                                                                                                                                                                                                                                                                                                      readable_rule  antecedent support  consequent support   support  confidence      lift  leverage  conviction\n",
            "1                                   (10543)                                                     (1440833)                                                                                                                                                                                                                                                        ((105, None, AF >= 0.80)) => ((14408, Misense, AF >= 0.80))            0.134585            0.951487  0.134585    1.000000  1.050987  0.006529         inf\n",
            "2                                   (10543)                                                     (1887713)                                                                                                                                                                                                                                                         ((105, None, AF >= 0.80)) => ((18877, Silent, AF >= 0.80))            0.134585            0.145540  0.134585    1.000000  6.870968  0.114998         inf\n",
            "3                                 (1887713)                                                       (10543)                                                                                                                                                                                                                                                         ((18877, Silent, AF >= 0.80)) => ((105, None, AF >= 0.80))            0.145540            0.134585  0.134585    0.924731  6.870968  0.114998   11.497653\n",
            "4                                   (10543)                                                     (2340333)                                                                                                                                                                                                                                                        ((105, None, AF >= 0.80)) => ((23403, Misense, AF >= 0.80))            0.134585            0.949922  0.134585    1.000000  1.052718  0.006740         inf\n",
            "6                                   (10543)                                                       (24143)                                                                                                                                                                                                                                                             ((105, None, AF >= 0.80)) => ((241, None, AF >= 0.80))            0.134585            0.948357  0.134585    1.000000  1.054455  0.006950         inf\n",
            "...                                     ...                                                           ...                                                                                                                                                                                                                                                                                                                ...                 ...                 ...       ...         ...       ...       ...         ...\n",
            "380343  (105933, 2587841, 1440833, 2355431)           (2556333, 24143, 2340333, 2654231, 2654531, 303713)  ((1059, Misense, AF >= 0.80) & ( 25878, None, AF < 0.20) & ( 14408, Misense, AF >= 0.80) & ( 23554, Misense, AF < 0.20)) => ((25563, Misense, AF >= 0.80) & ( 241, None, AF >= 0.80) & ( 23403, Misense, AF >= 0.80) & ( 26542, Misense, AF < 0.20) & ( 26545, Misense, AF < 0.20) & ( 3037, Silent, AF >= 0.80))            0.062598            0.190923  0.051643    0.825000  4.321107  0.039692    4.623295\n",
            "380345  (105933, 2587841, 2654531, 2355431)           (2556333, 24143, 2340333, 2654231, 1440833, 303713)  ((1059, Misense, AF >= 0.80) & ( 25878, None, AF < 0.20) & ( 26545, Misense, AF < 0.20) & ( 23554, Misense, AF < 0.20)) => ((25563, Misense, AF >= 0.80) & ( 241, None, AF >= 0.80) & ( 23403, Misense, AF >= 0.80) & ( 26542, Misense, AF < 0.20) & ( 14408, Misense, AF >= 0.80) & ( 3037, Silent, AF >= 0.80))            0.051643            0.211268  0.051643    1.000000  4.733333  0.040733         inf\n",
            "380346   (105933, 2587841, 2355431, 303713)          (2556333, 24143, 2340333, 2654231, 1440833, 2654531)  ((1059, Misense, AF >= 0.80) & ( 25878, None, AF < 0.20) & ( 23554, Misense, AF < 0.20) & ( 3037, Silent, AF >= 0.80)) => ((25563, Misense, AF >= 0.80) & ( 241, None, AF >= 0.80) & ( 23403, Misense, AF >= 0.80) & ( 26542, Misense, AF < 0.20) & ( 14408, Misense, AF >= 0.80) & ( 26545, Misense, AF < 0.20))            0.062598            0.190923  0.051643    0.825000  4.321107  0.039692    4.623295\n",
            "380456  (105933, 2355431, 2654531, 2654231)           (2587841, 2556333, 24143, 2340333, 1440833, 303713)  ((1059, Misense, AF >= 0.80) & ( 23554, Misense, AF < 0.20) & ( 26545, Misense, AF < 0.20) & ( 26542, Misense, AF < 0.20)) => ((25878, None, AF < 0.20) & ( 25563, Misense, AF >= 0.80) & ( 241, None, AF >= 0.80) & ( 23403, Misense, AF >= 0.80) & ( 14408, Misense, AF >= 0.80) & ( 3037, Silent, AF >= 0.80))            0.064163            0.217527  0.051643    0.804878  3.700123  0.037686    4.010172\n",
            "380508           (105933, 2587841, 2355431)  (2556333, 24143, 2340333, 2654231, 1440833, 2654531, 303713)  ((1059, Misense, AF >= 0.80) & ( 25878, None, AF < 0.20) & ( 23554, Misense, AF < 0.20)) => ((25563, Misense, AF >= 0.80) & ( 241, None, AF >= 0.80) & ( 23403, Misense, AF >= 0.80) & ( 26542, Misense, AF < 0.20) & ( 14408, Misense, AF >= 0.80) & ( 26545, Misense, AF < 0.20) & ( 3037, Silent, AF >= 0.80))            0.062598            0.190923  0.051643    0.825000  4.321107  0.039692    4.623295\n",
            "\n",
            "[100503 rows x 10 columns]\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3ASKXDBeOsr"
      },
      "source": [
        "---\n",
        "**MBA parameter**s \n",
        "\n",
        "1.   **bos_rules**: *support*=0.223, *confidence*=0.905, *lift*=2.863, *conviction*=7.000\n",
        "2.   **uke_rules**: *support*=0.210, *confidence*=0.910, *lift*=2.500, conviction *italicized text*=7.000\n",
        "3.   **ukl_rules**: *support*=0.203, *confidence*=0.926, *lift*=2.030, *conviction*=7.000\n",
        "---\n",
        "The last digit of an entry is Allele Frequency (**AF**)\n",
        "\n",
        "*   **3**: $>=$ 0.80    \n",
        "*   **2**: $>=$ 0.20 and $<$ 0.80\n",
        "*   **1**: $<$ 0.20\n",
        "---\n",
        "The digit before the last is **Funclass**\n",
        "\n",
        "*  **4**: None\n",
        "*  **3**: Missense\n",
        "*  **2**: Nonsense\n",
        "*  **1**: Silent\n",
        "---\n",
        "**Boston association rules** (12 rules)\n",
        "\n",
        "**Antecedent** entries\n",
        "\n",
        "* 3037**13**  (5 times)\n",
        "* 14408**33** (5 times)\n",
        "* 23403**33** (5 times)\n",
        "* 26542**31** (All 12)\n",
        "\n",
        "**Consequent** entries\n",
        "\n",
        "* 3037**13** (5 times)\n",
        "* 14408**33** (5 times)\n",
        "* 23403**33** (5 times)\n",
        "* 26545**31** (All 12)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XszXqioqokmE"
      },
      "source": [
        "---\n",
        "**UK early association rules** (10 rules)\n",
        "\n",
        "**Antecedent** entries\n",
        "\n",
        "* 241**43**   (9 times)\n",
        "* 14408**33** (3 times)\n",
        "* 23403**33** (4 times)\n",
        "* 28881**32** (All 10)\n",
        "\n",
        "**Consequent** entries\n",
        "\n",
        "* 3037**12**  (All 10)\n",
        "* 14408**33** (3 times)\n",
        "* 23403**33** (3 times)\n",
        "* 28883**33** (All 10)\n",
        "\n",
        "---\n",
        "**UK late association rules** (11 rules)\n",
        "\n",
        "**Antecedent** entries\n",
        "\n",
        "* 204**42** (All 11)\n",
        "* 445**13** (All 11)\n",
        "* 6286**13** (3 times)\n",
        "* 21255**13** (All 11)\n",
        "* 23403**33** (3 times)\n",
        "* 28932**32** (1 time)\n",
        "\n",
        "**Consequent** entries\n",
        "\n",
        "* 6286**13** (5 times)\n",
        "* 22227**32** (All 11)\n",
        "* 23403**33** (5 times)\n",
        "* 27944**13** (10 times)\n",
        "* 28932**32** (9 time)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTP92PqMymr8"
      },
      "source": [
        "**Entries that show up in antecedent/consequent across samples**\n",
        "\n",
        "**Antecedent**\n",
        "\n",
        "* 14408**33** (Boston, UKE)\n",
        "* 23403**33** (Boston, UKE, UKL)\n",
        "\n",
        "**Consequent**\n",
        "\n",
        "* 3037**13** (Boston, UKE)\n",
        "* 14408**33** (Boston, UKE)\n",
        "* 23403**33** (Boston, UKE, UKL)\n",
        "\n"
      ]
    }
  ]
}