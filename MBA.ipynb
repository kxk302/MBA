{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MBA",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kxk302/MBA/blob/main/MBA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JDt_5RMdjL7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51c89096-0a4b-40cc-f20a-c9f2e46a8081"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0bkibi4dy-V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f97c752b-96b3-4327-9cfc-8da52a7c2a42"
      },
      "source": [
        "!ls '/content/gdrive/MyDrive/Colab Notebooks/MBA_files'"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mba_input.csv  mba_output.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8BTJgr3VIiP"
      },
      "source": [
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from mlxtend.frequent_patterns import apriori\n",
        "from mlxtend.frequent_patterns import association_rules\n",
        "\n",
        "funclass_dict = {\"1\":\"Silent\", \"2\":\"Nonsense\", \"3\":\"Misense\", \"4\":\"None\"}\n",
        "af_dict = {\"1\":\"AF < 0.20\", \"2\": \"0.80 > AF >= 0.20\", \"3\":\"AF >= 0.80\"}\n",
        "\n",
        "# Method to display human readable rule, where absolute RNA position is translated to relative Protein position. \n",
        "def protein_relative_position(pos, offset):\n",
        "  return math.floor((pos - offset) / 3) + 1\n",
        "\n",
        "# Method to display human readable rule, where absolute RNA position is translated to relative Protein position.\n",
        "def to_str_each_translated(items, offset):\n",
        "  items_str = \"\"\n",
        "  for item in items:\n",
        "    item_str = \"(\" + str(protein_relative_position(int(item[0:-2:1]), offset)) + \", \" + funclass_dict.get(item[-2:-1:1]) + \", \" + af_dict.get(item[-1]) + \") & \"\n",
        "    items_str += item_str \n",
        "  return items_str[:-3]\n",
        "\n",
        "# Method to display human readable rule, where absolute RNA position is translated to relative Protein position.\n",
        "def to_str_translated(ser, offset):    \n",
        "  # Cast to string\n",
        "  ser = ser.astype(str)  \n",
        "\n",
        "  # Get rid of unnecessary characters prior to list of (position + funclass + af) numbers\n",
        "  ser = ser.str.slice(12,-3,1)  \n",
        "\n",
        "  # Get rid of single quotes\n",
        "  ser = ser.replace('\\'', '', regex=True)\n",
        "\n",
        "  ser = ser.str.split(pat=\",\")\n",
        "\n",
        "  return ser.apply(to_str_each_translated, args=(offset,))\n",
        "\n",
        "# Method to display human readable rule, where absolute RNA position is translated to relative Protein position.\n",
        "def add_readable_rules_translated(df_in, offset):\n",
        "  # Empty data frame\n",
        "  if df_in is None or df_in.shape[0] == 0:\n",
        "    return df_in\n",
        "\n",
        "  df = df_in.copy()\n",
        "\n",
        "  antecedents_str = to_str_translated(df['antecedents'], offset)\n",
        "  consequents_str = to_str_translated(df['consequents'], offset)\n",
        "\n",
        "  readable_rule = \"(\" + antecedents_str + \") => (\" + consequents_str + \")\"\n",
        "  df.insert(2,'translated_readable_rule', readable_rule)\n",
        "\n",
        "  return df\n",
        "\n",
        "# Method to display human readable rule\n",
        "def to_str_each(items):\n",
        "  items_str = \"\"\n",
        "  for item in items:\n",
        "    item_str = \"(\" + item[0:-2:1] + \", \" + funclass_dict.get(item[-2:-1:1]) + \", \" + af_dict.get(item[-1]) + \") & \"\n",
        "    items_str += item_str \n",
        "  return items_str[:-3]\n",
        "\n",
        "# Method to display human readable rule\n",
        "def to_str(ser):    \n",
        "  # Cast to string\n",
        "  ser = ser.astype(str)  \n",
        "\n",
        "  # Get rid of unnecessary characters prior to list of (position + funclass + af) numbers\n",
        "  ser = ser.str.slice(12,-3,1)  \n",
        "\n",
        "  # Get rid of single quotes\n",
        "  ser = ser.replace('\\'', '', regex=True)\n",
        "\n",
        "  ser = ser.str.split(pat=\",\")\n",
        "\n",
        "  return ser.apply(to_str_each)\n",
        "\n",
        "# Method to display human readable rule\n",
        "def add_readable_rules(df_in):\n",
        "  # Empty data frame\n",
        "  if df_in is None or df_in.shape[0] == 0:\n",
        "    return df_in\n",
        "\n",
        "  df = df_in.copy()\n",
        "\n",
        "  antecedents_str = to_str(df['antecedents'])\n",
        "  consequents_str = to_str(df['consequents'])\n",
        "\n",
        "  readable_rule = \"(\" + antecedents_str + \") => (\" + consequents_str + \")\"\n",
        "  df.insert(2,'readable_rule', readable_rule)\n",
        "\n",
        "  return df\n",
        "\n",
        "# Filter data based on probability of position in the dataset. Filter specified as a range.\n",
        "def filter_on_position_probability(df_in, start_prob=None, end_prob=None):\n",
        "    if df_in is None or df_in.shape[0] == 0:\n",
        "      return df_in\n",
        "\n",
        "    if start_prob is None and end_prob is None:\n",
        "      return df_in\n",
        "\n",
        "    # Only consider rows where probability of Position being present in the Samples is >= start_prob and <= end_prob\n",
        "    num_samples = df_in['Sample'].nunique()\n",
        "    print(\"num_samples: {}\".format(num_samples))\n",
        "\n",
        "    num_positions = df_in['POS'].nunique()\n",
        "    print(\"num_positions: {}\".format(num_positions))\n",
        "   \n",
        "    df = df_in[['Sample', 'POS']].groupby(['POS']).agg(position_prob=('Sample', 'count'))\n",
        "    # Normalize position_prob by dividing it by num_samples. This makes it a proability between 0.0 and 1.0\n",
        "    df['position_prob'] = df['position_prob'] / num_samples   \n",
        "\n",
        "    df_sorted = df.sort_values(by = 'position_prob')\n",
        "    print(df_sorted.head())\n",
        "    print(df_sorted.tail())\n",
        "  \n",
        "    if start_prob is not None:\n",
        "      df = df[df.position_prob >= start_prob]\n",
        "\n",
        "    if end_prob is not None:\n",
        "      df = df[df.position_prob <= end_prob]\n",
        "    \n",
        "    ret_val = pd.merge(df_in, df, on='POS')\n",
        "\n",
        "    df_sorted = ret_val.sort_values(by = 'position_prob')[['POS', 'position_prob']]\n",
        "    print(df_sorted.head())\n",
        "    print(df_sorted.tail())\n",
        "\n",
        "    return ret_val\n",
        "\n",
        "# Return the probbility of a position. For testing/verification purposes.\n",
        "def get_position_probability(df_in, position=None):\n",
        "    if df_in is None or df_in.shape[0] == 0:\n",
        "      return df_in\n",
        "\n",
        "    if position is None:\n",
        "      return df_in\n",
        "\n",
        "    num_samples = df_in['Sample'].nunique()\n",
        "    print(\"num_samples: {}\".format(num_samples))\n",
        "\n",
        "    num_positions = df_in['POS'].nunique()\n",
        "    print(\"num_positions: {}\".format(num_positions))\n",
        "   \n",
        "    df = df_in[['Sample', 'POS']].groupby(['POS']).agg(position_prob=('Sample', 'count'))\n",
        "    # Normalize position_prob by dividing it by num_samples. This makes it a proability between 0.0 and 1.0\n",
        "    df['position_prob'] = df['position_prob'] / num_samples\n",
        "\n",
        "    try:      \n",
        "      return df.loc[position,'position_prob']\n",
        "    except KeyError:\n",
        "      return -1\n",
        "\n",
        "# Filter data based on values of position, AF, and funclass. Position and AF filter specified as a range. funclass filter is a list of acceptable values.\n",
        "def filter_on_column_values(df_in, start_pos=None, end_pos=None, start_af=None, end_af=None, funclass=[]):  \n",
        "  if df_in is None or df_in.shape[0] == 0:\n",
        "    return df_in\n",
        "\n",
        "  df = df_in.copy()\n",
        "\n",
        "  # Replace \".\" with \"NONE\" in FUNCLASS column. They both represent \"Non-coding\" variant\n",
        "  df[\"FUNCLASS\"] = df[\"FUNCLASS\"].replace('.', 'NONE')\n",
        "\n",
        "  # Filter in_file rows\n",
        "  if start_pos is not None:\n",
        "    df = df[df.POS >= start_pos]    \n",
        "  if end_pos is not None:\n",
        "    df = df[df.POS < end_pos]    \n",
        "  if start_af is not None:\n",
        "    df = df[df.AF >= start_af]    \n",
        "  if end_af is not None:\n",
        "    df = df[df.AF < end_af]    \n",
        "  if len(funclass) > 0:    \n",
        "    df = df[df.FUNCLASS.isin(funclass)]\n",
        "\n",
        "  return df\n",
        "\n",
        "# Create a single integer representing a variant at a specific position with a specific allele frequency\n",
        "# Pivot the data so we have all sample variants on a single line\n",
        "#\n",
        "# Extract rows from in_file where\n",
        "# \n",
        "#    POS >= start_pos & POS <= end_pos\n",
        "#        If start_pos = None: POS <= end_pos\n",
        "#        If end_pos = None: POS >= start_pos \n",
        "#\n",
        "#    AF >= start_af & AF <= end_af\n",
        "#        If start_af = None: AF <= end_af\n",
        "#        If end_af = None: AF >= start_af\n",
        "#\n",
        "#    FUNCLASS in funclass (funclass is a list)      \n",
        "#\n",
        "def preprocess_input_file(df_in):\n",
        "  if df_in is None or df_in.shape[0] == 0:\n",
        "    return df_in\n",
        "  \n",
        "  df = df_in.copy()\n",
        "\n",
        "  # Bucket values in FUNCLASS and AF columns. We do not bucket the values in POS column.\n",
        "\n",
        "  # Replace variants in FUNCLASS column with a distinct numeric value\n",
        "  df.loc[df.FUNCLASS == \"NONE\", \"FUNCLASS\"] = 4\n",
        "  df.loc[df.FUNCLASS == \"MISSENSE\", \"FUNCLASS\"] = 3\n",
        "  df.loc[df.FUNCLASS == \"NONSENSE\", \"FUNCLASS\"] = 2 \n",
        "  df.loc[df.FUNCLASS == \"SILENT\", \"FUNCLASS\"] = 1 \n",
        "\n",
        "  # Replace allele frequency in AF column with a distict numeric value\n",
        "  df.loc[df.AF >= 0.80, \"AF\"] = 3\n",
        "  df.loc[(df.AF >= 0.20) & (df.AF < 0.80), \"AF\"] = 2\n",
        "  df.loc[df.AF < 0.20, \"AF\"] = 1\n",
        "\n",
        "  # Convert AF values to integer\n",
        "  df = df.astype({\"AF\": int}) \n",
        "\n",
        "  # Create a new column called 'Label', which is a string concatentation of POS, FUNCLASS, and AF values. \n",
        "  # The idea is to represent each variant + allele frequency + position as a single integer, to be used in MBA \n",
        "  df[\"Label\"] = df[\"POS\"].astype(str) + df[\"FUNCLASS\"].astype(str) + df[\"AF\"].astype(str)\n",
        "\n",
        "  # We do not need POS, FUNCLASS, and AF columns anymore\n",
        "  df = df[[\"Sample\", \"Label\"]]\n",
        "  \n",
        "  # Add a new column called 'Value', prepopulated with 1\n",
        "  df[\"Value\"] = 1\n",
        "\n",
        "  df = pd.pivot_table(df, index=\"Sample\", columns=\"Label\", values=\"Value\")\n",
        "\n",
        "  # Set all data frame nan (not a number) values to 0\n",
        "  df = df.fillna(0)\n",
        "\n",
        "  # Convert all data framevalues to integer\n",
        "  df = df.astype(int) \n",
        "\n",
        "  return df"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VubP1ctVUsc"
      },
      "source": [
        "def get_association_rules(in_file, min_support=0.20, min_confidence=0.80, min_lift=1.0, \n",
        "                          min_conviction=1.0, max_len=None, start_pos=None, end_pos=None, \n",
        "                          start_af=None, end_af=None, funclass=[], start_prob=None, end_prob=None):\n",
        "  \n",
        "  # Read the input file and pick the needed columns\n",
        "  df_in = pd.read_csv(in_file, sep='\\t')[['Sample', 'POS', 'AF', 'FUNCLASS']]\n",
        "\n",
        "  # Filter on position probability\n",
        "  df_pp = filter_on_position_probability(df_in, start_prob, end_prob)\n",
        "\n",
        "  # Filter on column values\n",
        "  df_cv = filter_on_column_values(df_pp, start_pos, end_pos, start_af, end_af, funclass)\n",
        "\n",
        "  # Preprocess the data frame\n",
        "  df = preprocess_input_file(df_cv)\n",
        "\n",
        "  # Get frequent item sets, with support larger than min_support, using Apriori algorithm\n",
        "  frequent_itemsets = apriori(df, min_support=min_support, use_colnames=True, max_len=max_len)\n",
        "\n",
        "  # Get association rules, with lift larger than min_lift  \n",
        "  rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=min_lift)\n",
        "\n",
        "  # Filter association rules, keeping rules with confidence larger than min_confidence\n",
        "  rules = rules[ (rules['confidence'] >= min_confidence) & (rules['conviction'] >= min_conviction) ]\n",
        "\n",
        "  return rules\n",
        "\n",
        "def get_association_rules_param(param_dict):\n",
        "  in_file = param_dict.get('in_file', None)\n",
        "  min_support = param_dict.get('min_support', 0.20)\n",
        "  min_confidence = param_dict.get('min_confidence', 0.80)\n",
        "  min_lift = param_dict.get('min_lift', 1.0)\n",
        "  min_conviction = param_dict.get('min_conviction', 1.0)\n",
        "  max_len = param_dict.get('max_len', None)\n",
        "  start_pos = param_dict.get('start_pos', None)\n",
        "  end_pos = param_dict.get('end_pos', None)\n",
        "  start_af = param_dict.get('start_af', None)\n",
        "  end_af = param_dict.get('end_af', None)\n",
        "  funclass = param_dict.get('funclass', [])\n",
        "  start_prob = param_dict.get('start_prob', None)\n",
        "  end_prob = param_dict.get('end_prob', None)\n",
        "\n",
        "  return get_association_rules(in_file, min_support, min_confidence, min_lift, \n",
        "                               min_conviction, max_len, start_pos, end_pos, \n",
        "                               start_af, end_af, funclass, start_prob, end_prob)\n",
        "             "
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxoxfSG9fRA3"
      },
      "source": [
        "# Original analysis\n",
        "pd.set_option('max_columns', 10, 'display.expand_frame_repr', False)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "bos_rules = get_association_rules(in_file=\"https://github.com/galaxyproject/SARS-CoV-2/raw/master/data/var/bos_by_sample.tsv.gz\", \n",
        "                                  min_support=0.223, min_confidence=0.905, min_lift=2.863, min_conviction=7.0)\n",
        "num_rules = bos_rules.shape[0]\n",
        "print('Boston dataset association rules: ')\n",
        "bos_rules = add_readable_rules(bos_rules)\n",
        "print(bos_rules.head(num_rules))\n",
        "print('\\n\\n')\n",
        "# bos_rules.to_csv('/content/gdrive/MyDrive/Colab Notebooks/MBA_files/bos_0223_0905_2863_7000.csv', sep=',')\n",
        "\n",
        "uke_rules = get_association_rules(in_file=\"https://github.com/galaxyproject/SARS-CoV-2/raw/master/data/var/cog_20200917_by_sample.tsv.gz\", \n",
        "                                  min_support=0.21, min_confidence=0.91, min_lift=2.5, min_conviction=7.0)\n",
        "num_rules = uke_rules.shape[0]\n",
        "print('UK early dataset association rules: ')\n",
        "uke_rules = add_readable_rules(uke_rules)\n",
        "print(uke_rules.head(num_rules))\n",
        "print('\\n\\n')\n",
        "# uke_rules.to_csv('/content/gdrive/MyDrive/Colab Notebooks/MBA_files/uke_0210_0910_2500_7000.csv', sep=',')\n",
        "\n",
        "ukl_rules = get_association_rules(in_file=\"https://github.com/galaxyproject/SARS-CoV-2/raw/master/data/var/cog_20201120_by_sample.tsv.gz\", \n",
        "                                  min_support=0.203, min_confidence=0.926, min_lift=2.03, min_conviction=7.0)\n",
        "num_rules = ukl_rules.shape[0]\n",
        "print('Uk late dataset association rules: ')\n",
        "ukl_rules = add_readable_rules(ukl_rules)\n",
        "print(ukl_rules.head(num_rules))\n",
        "print('\\n\\n')\n",
        "# ukl_rules.to_csv('/content/gdrive/MyDrive/Colab Notebooks/MBA_files/ukl_0203_0926_2030_7000.csv', sep=',')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uy3YlHqEyPGK"
      },
      "source": [
        "# Milad \n",
        "pd.set_option('max_columns', 11, 'display.expand_frame_repr', False)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# floor(ABS_RNA - 21563) / 3) + 1= REL_PROT\n",
        "# (REL_PROT - 1) * 3 + 21563 + ABS_RNA \n",
        "# \n",
        "# S protein start and end positions: 21563 to 25384\n",
        "# 22571: Leu aminoacid\n",
        "# \n",
        "\n",
        "param_dict = {\n",
        "    \"in_file\":\"https://github.com/galaxyproject/SARS-CoV-2/raw/master/data/var/bos_by_sample.tsv.gz\", \n",
        "    \"min_support\": 0.005, \n",
        "    \"min_confidence\": 0.01, \n",
        "    \"min_lift\": 1.0, \n",
        "    \"min_conviction\": 1.0, \n",
        "    \"max_len\": 2, \n",
        "    \"start_pos\": 21563,\n",
        "    \"end_pos\": 25385,\n",
        "    \"start_af\": 0.20,\n",
        "    \"end_af\": 1.00, \n",
        "    \"start_prob\": 0.000, \n",
        "    \"end_prob\": 0.100, \n",
        "    \"funclass\": [\"MISSENSE\"]\n",
        "}\n",
        "'''\n",
        "bos_rules = get_association_rules(in_file=\"https://github.com/galaxyproject/SARS-CoV-2/raw/master/data/var/bos_by_sample.tsv.gz\", min_support=0.005, \n",
        "                                  min_confidence=0.01, min_lift=1.00, min_conviction=1.00, max_len=2, start_pos=21563, end_pos=25385, start_af=0.20, \n",
        "                                  end_af=1.00, start_prob=0.000, end_prob=0.100, funclass=[\"MISSENSE\"])\n",
        "'''\n",
        "bos_rules = get_association_rules_param(param_dict)\n",
        "num_rules = bos_rules.shape[0]\n",
        "print('Boston dataset association rules: ')\n",
        "bos_rules = add_readable_rules(bos_rules)\n",
        "bos_rules = add_readable_rules_translated(bos_rules, 21563)\n",
        "print(bos_rules.head(num_rules))\n",
        "print('\\n\\n')\n",
        "# bos_rules.to_csv('/content/gdrive/MyDrive/Colab Notebooks/MBA_files/bos_0223_0905_2863_7000.csv', sep=',')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5erF_k0uKLI"
      },
      "source": [
        "# Anton\n",
        "pd.set_option('max_columns', 10, 'display.expand_frame_repr', False)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "param_dict = {\n",
        "    \"in_file\":\"https://github.com/galaxyproject/SARS-CoV-2/raw/master/data/var/bos_by_sample.tsv.gz\", \n",
        "    \"min_support\": 0.050, \n",
        "    \"min_confidence\": 0.800, \n",
        "    \"min_lift\": 1.0, \n",
        "    \"min_conviction\": 1.0, \n",
        "    \"max_len\": 6, \n",
        "    \"end_af\": 0.80, \n",
        "    \"start_prob\": 0.000, \n",
        "    \"end_prob\": 0.250, \n",
        "    \"funclass\": [\"MISSENSE\", \"SILENT\"]\n",
        "}\n",
        "'''\n",
        "bos_rules = get_association_rules(in_file=\"https://github.com/galaxyproject/SARS-CoV-2/raw/master/data/var/bos_by_sample.tsv.gz\", \n",
        "                                  min_support=0.050, min_confidence=0.800, min_lift=1.0, min_conviction=1.0, max_len=6, end_af=0.80, \n",
        "                                  start_prob=0.000, end_prob=0.250, funclass=[\"MISSENSE\", \"SILENT\"])\n",
        "'''\n",
        "bos_rules = get_association_rules_param(param_dict)\n",
        "num_rules = bos_rules.shape[0]\n",
        "print('Number of rules: {}'.format(num_rules))\n",
        "print('Boston dataset association rules: ')\n",
        "bos_rules = add_readable_rules(bos_rules)\n",
        "print(bos_rules.head(num_rules))\n",
        "print('\\n\\n')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEa5yn9JFJDI"
      },
      "source": [
        "# Test get_position_probability()\n",
        "\n",
        "pd.set_option('max_columns', 10, 'display.expand_frame_repr', False)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# Read the input file and pick the needed columns\n",
        "in_file = \"https://github.com/galaxyproject/SARS-CoV-2/raw/master/data/var/bos_by_sample.tsv.gz\"\n",
        "df_in = pd.read_csv(in_file, sep='\\t')[['Sample', 'POS', 'AF', 'FUNCLASS']]\n",
        "\n",
        "position=23403\n",
        "position_prob = get_position_probability(df_in, position=position)\n",
        "print(\"position: {}, position_prob: {:.4f}\".format(position, position_prob))\n",
        "\n",
        "position=23403\n",
        "position_prob = get_position_probability(df_in, position=position)\n",
        "print(\"position: {}, position_prob: {:.4f}\".format(position, position_prob))\n",
        "\n",
        "position=23403\n",
        "position_prob = get_position_probability(df_in, position=position)\n",
        "print(\"position: {}, position_prob: {:.4f}\".format(position, position_prob))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wf7flupelghL"
      },
      "source": [
        "# Testing Galaxy wrapper for MBA\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from mlxtend.frequent_patterns import apriori, association_rules\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "\n",
        "pd.set_option('max_columns', 10, 'display.expand_frame_repr', False)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "in_file = \"/content/gdrive/MyDrive/Colab Notebooks/MBA_files/mba_input.csv\"\n",
        "out_file = \"/content/gdrive/MyDrive/Colab Notebooks/MBA_files/mba_output.csv\"\n",
        "max_len = 4\n",
        "min_support=0.6\n",
        "min_confidence=0.6\n",
        "min_lift=1.0\n",
        "min_conviction=1.0\n",
        "\n",
        "with open(in_file) as fp:\n",
        "    lines = fp.read().splitlines() \n",
        "\n",
        "dataset = []\n",
        "for line in lines:\n",
        "  line_items = line.split(\",\")\n",
        "  dataset.append(line_items)\n",
        "  \n",
        "te = TransactionEncoder()\n",
        "te_ary = te.fit_transform(dataset)\n",
        "\n",
        "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
        "\n",
        "frequent_itemsets = apriori(df, min_support=min_support, use_colnames=True, max_len=max_len)\n",
        "\n",
        "# Get association rules, with lift larger than min_lift  \n",
        "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=min_confidence)\n",
        "\n",
        "# Filter association rules, keeping rules with lift and conviction larger than min_liftand min_conviction\n",
        "rules = rules[ (rules['lift'] >= min_lift) & (rules['conviction'] >= min_conviction) ]\n",
        "\n",
        "rules['antecedents'] = rules['antecedents'].apply(list)\n",
        "rules['consequents'] = rules['consequents'].apply(list)\n",
        "\n",
        "rules.to_csv(out_file)"
      ],
      "execution_count": 42,
      "outputs": []
    }
  ]
}