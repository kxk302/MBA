{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MBA",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kxk302/MBA/blob/main/MBA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JDt_5RMdjL7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0bkibi4dy-V"
      },
      "source": [
        "!ls '/content/gdrive/MyDrive/Colab Notebooks/MBA_files'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8BTJgr3VIiP"
      },
      "source": [
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from mlxtend.frequent_patterns import apriori\n",
        "from mlxtend.frequent_patterns import association_rules\n",
        "\n",
        "funclass_dict = {\"1\":\"Silent\", \"2\":\"Nonsense\", \"3\":\"Misense\", \"4\":\"None\"}\n",
        "af_dict = {\"1\":\"AF < 0.20\", \"2\": \"0.80 > AF >= 0.20\", \"3\":\"AF >= 0.80\"}\n",
        "\n",
        "def protein_relative_position(pos, offset):\n",
        "  return math.floor((pos - offset) / 3) + 1\n",
        "\n",
        "def to_str_each_translated(items, offset):\n",
        "  items_str = \"\"\n",
        "  for item in items:\n",
        "    item_str = \"(\" + str(protein_relative_position(int(item[0:-2:1]), offset)) + \", \" + funclass_dict.get(item[-2:-1:1]) + \", \" + af_dict.get(item[-1]) + \") & \"\n",
        "    items_str += item_str \n",
        "  return items_str[:-3]\n",
        "\n",
        "# Convert series of antecedents/consequents to human readable \n",
        "def to_str_translated(ser, offset):    \n",
        "  # Cast to string\n",
        "  ser = ser.astype(str)  \n",
        "\n",
        "  # Get rid of unnecessary characters prior to list of (position + funclass + af) numbers\n",
        "  ser = ser.str.slice(12,-3,1)  \n",
        "\n",
        "  # Get rid of single quotes\n",
        "  ser = ser.replace('\\'', '', regex=True)\n",
        "\n",
        "  ser = ser.str.split(pat=\",\")\n",
        "\n",
        "  return ser.apply(to_str_each_translated, args=(offset,))\n",
        "\n",
        "def add_readable_rules_translated(df_in, offset):\n",
        "  # Empty data frame\n",
        "  if df_in is None or df_in.shape[0] == 0:\n",
        "    return df_in\n",
        "\n",
        "  df = df_in.copy()\n",
        "\n",
        "  antecedents_str = to_str_translated(df['antecedents'], offset)\n",
        "  consequents_str = to_str_translated(df['consequents'], offset)\n",
        "\n",
        "  readable_rule = \"(\" + antecedents_str + \") => (\" + consequents_str + \")\"\n",
        "  df.insert(2,'translated_readable_rule', readable_rule)\n",
        "\n",
        "  return df\n",
        "\n",
        "def to_str_each(items):\n",
        "  items_str = \"\"\n",
        "  for item in items:\n",
        "    item_str = \"(\" + item[0:-2:1] + \", \" + funclass_dict.get(item[-2:-1:1]) + \", \" + af_dict.get(item[-1]) + \") & \"\n",
        "    items_str += item_str \n",
        "  return items_str[:-3]\n",
        "\n",
        "# Convert series of antecedents/consequents to human readable \n",
        "def to_str(ser):    \n",
        "  # Cast to string\n",
        "  ser = ser.astype(str)  \n",
        "\n",
        "  # Get rid of unnecessary characters prior to list of (position + funclass + af) numbers\n",
        "  ser = ser.str.slice(12,-3,1)  \n",
        "\n",
        "  # Get rid of single quotes\n",
        "  ser = ser.replace('\\'', '', regex=True)\n",
        "\n",
        "  ser = ser.str.split(pat=\",\")\n",
        "\n",
        "  return ser.apply(to_str_each)\n",
        "\n",
        "def add_readable_rules(df_in):\n",
        "  # Empty data frame\n",
        "  if df_in is None or df_in.shape[0] == 0:\n",
        "    return df_in\n",
        "\n",
        "  df = df_in.copy()\n",
        "\n",
        "  antecedents_str = to_str(df['antecedents'])\n",
        "  consequents_str = to_str(df['consequents'])\n",
        "\n",
        "  readable_rule = \"(\" + antecedents_str + \") => (\" + consequents_str + \")\"\n",
        "  df.insert(2,'readable_rule', readable_rule)\n",
        "\n",
        "  return df\n",
        "\n",
        "def filter_on_position_probability(df_in, start_prob=None, end_prob=None):\n",
        "    if df_in is None or df_in.shape[0] == 0:\n",
        "      return df_in\n",
        "\n",
        "    if start_prob is None and end_prob is None:\n",
        "      return df_in\n",
        "\n",
        "    # Only consider rows where probability of Position being present in the Samples is >= start_prob and <= end_prob\n",
        "    num_samples = df_in['Sample'].nunique()\n",
        "    print(\"num_samples: {}\".format(num_samples))\n",
        "\n",
        "    num_positions = df_in['POS'].nunique()\n",
        "    print(\"num_positions: {}\".format(num_positions))\n",
        "   \n",
        "    df = df_in[['Sample', 'POS']].groupby(['POS']).agg(position_prob=('Sample', 'count'))\n",
        "    # Normalize position_prob by dividing it by num_samples. This makes it a proability between 0.0 and 1.0\n",
        "    df['position_prob'] = df['position_prob'] / num_samples   \n",
        "\n",
        "    df_sorted = df.sort_values(by = 'position_prob')\n",
        "    print(df_sorted.head())\n",
        "    print(df_sorted.tail())\n",
        "  \n",
        "    if start_prob is not None:\n",
        "      df = df[df.position_prob >= start_prob]\n",
        "\n",
        "    if end_prob is not None:\n",
        "      df = df[df.position_prob <= end_prob]\n",
        "    \n",
        "    ret_val = pd.merge(df_in, df, on='POS')\n",
        "\n",
        "    df_sorted = ret_val.sort_values(by = 'position_prob')[['POS', 'position_prob']]\n",
        "    print(df_sorted.head())\n",
        "    print(df_sorted.tail())\n",
        "\n",
        "    return ret_val\n",
        "\n",
        "def get_position_probability(df_in, position=None):\n",
        "    if df_in is None or df_in.shape[0] == 0:\n",
        "      return df_in\n",
        "\n",
        "    if position is None:\n",
        "      return df_in\n",
        "\n",
        "    num_samples = df_in['Sample'].nunique()\n",
        "    print(\"num_samples: {}\".format(num_samples))\n",
        "\n",
        "    num_positions = df_in['POS'].nunique()\n",
        "    print(\"num_positions: {}\".format(num_positions))\n",
        "   \n",
        "    df = df_in[['Sample', 'POS']].groupby(['POS']).agg(position_prob=('Sample', 'count'))\n",
        "    # Normalize position_prob by dividing it by num_samples. This makes it a proability between 0.0 and 1.0\n",
        "    df['position_prob'] = df['position_prob'] / num_samples\n",
        "\n",
        "    try:      \n",
        "      return df.loc[position,'position_prob']\n",
        "    except KeyError:\n",
        "      return -1\n",
        "\n",
        "def filter_on_column_values(df_in, start_pos=None, end_pos=None, start_af=None, end_af=None, funclass=[]):  \n",
        "  if df_in is None or df_in.shape[0] == 0:\n",
        "    return df_in\n",
        "\n",
        "  df = df_in.copy()\n",
        "\n",
        "  # Replace \".\" with \"NONE\" in FUNCLASS column. They both represent \"Non-coding\" variant\n",
        "  df[\"FUNCLASS\"] = df[\"FUNCLASS\"].replace('.', 'NONE')\n",
        "\n",
        "  # Filter in_file rows\n",
        "  if start_pos is not None:\n",
        "    df = df[df.POS >= start_pos]    \n",
        "  if end_pos is not None:\n",
        "    df = df[df.POS < end_pos]    \n",
        "  if start_af is not None:\n",
        "    df = df[df.AF >= start_af]    \n",
        "  if end_af is not None:\n",
        "    df = df[df.AF < end_af]    \n",
        "  if len(funclass) > 0:    \n",
        "    df = df[df.FUNCLASS.isin(funclass)]\n",
        "\n",
        "  return df\n",
        "\n",
        "# Create a single integer representing a variant at a specific position with a specific allele frequency\n",
        "# Pivot the data so we have all sample variants on a single line\n",
        "#\n",
        "# Extract rows from in_file where\n",
        "# \n",
        "#    POS >= start_pos & POS <= end_pos\n",
        "#        If start_pos = None: POS <= end_pos\n",
        "#        If end_pos = None: POS >= start_pos \n",
        "#\n",
        "#    AF >= start_af & AF <= end_af\n",
        "#        If start_af = None: AF <= end_af\n",
        "#        If end_af = None: AF >= start_af\n",
        "#\n",
        "#    FUNCLASS in funclass (funclass is a list)      \n",
        "#\n",
        "def preprocess_input_file(df_in):\n",
        "  if df_in is None or df_in.shape[0] == 0:\n",
        "    return df_in\n",
        "  \n",
        "  df = df_in.copy()\n",
        "\n",
        "  # Bucket values in FUNCLASS and AF columns. We do not bucket the values in POS column.\n",
        "\n",
        "  # Replace variants in FUNCLASS column with a distinct numeric value\n",
        "  df.loc[df.FUNCLASS == \"NONE\", \"FUNCLASS\"] = 4\n",
        "  df.loc[df.FUNCLASS == \"MISSENSE\", \"FUNCLASS\"] = 3\n",
        "  df.loc[df.FUNCLASS == \"NONSENSE\", \"FUNCLASS\"] = 2 \n",
        "  df.loc[df.FUNCLASS == \"SILENT\", \"FUNCLASS\"] = 1 \n",
        "\n",
        "  # Replace allele frequency in AF column with a distict numeric value\n",
        "  df.loc[df.AF >= 0.80, \"AF\"] = 3\n",
        "  df.loc[(df.AF >= 0.20) & (df.AF < 0.80), \"AF\"] = 2\n",
        "  df.loc[df.AF < 0.20, \"AF\"] = 1\n",
        "\n",
        "  # Convert AF values to integer\n",
        "  df = df.astype({\"AF\": int}) \n",
        "\n",
        "  # Create a new column called 'Label', which is a string concatentation of POS, FUNCLASS, and AF values. \n",
        "  # The idea is to represent each variant + allele frequency + position as a single integer, to be used in MBA \n",
        "  df[\"Label\"] = df[\"POS\"].astype(str) + df[\"FUNCLASS\"].astype(str) + df[\"AF\"].astype(str)\n",
        "\n",
        "  # We do not need POS, FUNCLASS, and AF columns anymore\n",
        "  df = df[[\"Sample\", \"Label\"]]\n",
        "  \n",
        "  # Add a new column called 'Value', prepopulated with 1\n",
        "  df[\"Value\"] = 1\n",
        "\n",
        "  df = pd.pivot_table(df, index=\"Sample\", columns=\"Label\", values=\"Value\")\n",
        "\n",
        "  # Set all data frame nan (not a number) values to 0\n",
        "  df = df.fillna(0)\n",
        "\n",
        "  # Convert all data framevalues to integer\n",
        "  df = df.astype(int) \n",
        "\n",
        "  return df"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VubP1ctVUsc"
      },
      "source": [
        "def get_association_rules(in_file, min_support=0.20, min_confidence=0.80, min_lift=1.0, min_conviction=1.0, max_len=None, start_pos=None, end_pos=None, start_af=None, end_af=None, funclass=[], start_prob=None, end_prob=None):\n",
        "  # Read the input file and pick the needed columns\n",
        "  df_in = pd.read_csv(in_file, sep='\\t')[['Sample', 'POS', 'AF', 'FUNCLASS']]\n",
        "\n",
        "  # Filter on position probability\n",
        "  df_pp = filter_on_position_probability(df_in, start_prob, end_prob)\n",
        "\n",
        "  # Filter on column values\n",
        "  df_cv = filter_on_column_values(df_pp, start_pos, end_pos, start_af, end_af, funclass)\n",
        "\n",
        "  # Preprocess the data frame\n",
        "  df = preprocess_input_file(df_cv)\n",
        "\n",
        "  # Get frequent item sets, with support larger than min_support, using Apriori algorithm\n",
        "  frequent_itemsets = apriori(df, min_support=min_support, use_colnames=True, max_len=max_len)\n",
        "\n",
        "  # Get association rules, with lift larger than min_lift  \n",
        "  rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=min_lift)\n",
        "\n",
        "  # Filter association rules, keeping rules with confidence larger than min_confidence\n",
        "  rules = rules[ (rules['confidence'] >= min_confidence) & (rules['conviction'] >= min_conviction) ]\n",
        "\n",
        "  return rules"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxoxfSG9fRA3"
      },
      "source": [
        "pd.set_option('max_columns', 10, 'display.expand_frame_repr', False)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "bos_rules = get_association_rules(in_file=\"https://github.com/galaxyproject/SARS-CoV-2/raw/master/data/var/bos_by_sample.tsv.gz\", min_support=0.223, min_confidence=0.905, min_lift=2.863, min_conviction=7.0)\n",
        "num_rules = bos_rules.shape[0]\n",
        "print('Boston dataset association rules: ')\n",
        "bos_rules = add_readable_rules(bos_rules)\n",
        "print(bos_rules.head(num_rules))\n",
        "print('\\n\\n')\n",
        "# bos_rules.to_csv('/content/gdrive/MyDrive/Colab Notebooks/MBA_files/bos_0223_0905_2863_7000.csv', sep=',')\n",
        "\n",
        "uke_rules = get_association_rules(in_file=\"https://github.com/galaxyproject/SARS-CoV-2/raw/master/data/var/cog_20200917_by_sample.tsv.gz\", min_support=0.21, min_confidence=0.91, min_lift=2.5, min_conviction=7.0)\n",
        "num_rules = uke_rules.shape[0]\n",
        "print('UK early dataset association rules: ')\n",
        "uke_rules = add_readable_rules(uke_rules)\n",
        "print(uke_rules.head(num_rules))\n",
        "print('\\n\\n')\n",
        "# uke_rules.to_csv('/content/gdrive/MyDrive/Colab Notebooks/MBA_files/uke_0210_0910_2500_7000.csv', sep=',')\n",
        "\n",
        "ukl_rules = get_association_rules(in_file=\"https://github.com/galaxyproject/SARS-CoV-2/raw/master/data/var/cog_20201120_by_sample.tsv.gz\", min_support=0.203, min_confidence=0.926, min_lift=2.03, min_conviction=7.0)\n",
        "num_rules = ukl_rules.shape[0]\n",
        "print('Uk late dataset association rules: ')\n",
        "ukl_rules = add_readable_rules(ukl_rules)\n",
        "print(ukl_rules.head(num_rules))\n",
        "print('\\n\\n')\n",
        "# ukl_rules.to_csv('/content/gdrive/MyDrive/Colab Notebooks/MBA_files/ukl_0203_0926_2030_7000.csv', sep=',')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uy3YlHqEyPGK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9babf448-2238-4905-ed84-a686c857e7f5"
      },
      "source": [
        "# Milad \n",
        "pd.set_option('max_columns', 11, 'display.expand_frame_repr', False)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# floor(ABS_RNA - 21563) / 3) + 1= REL_PROT\n",
        "# (REL_PROT - 1) * 3 + 21563 + ABS_RNA \n",
        "# \n",
        "# S protein start and end positions: 21563 to 25384\n",
        "# 22571: Leu aminoacid\n",
        "# \n",
        "\n",
        "bos_rules = get_association_rules(in_file=\"https://github.com/galaxyproject/SARS-CoV-2/raw/master/data/var/bos_by_sample.tsv.gz\", min_support=0.005, min_confidence=0.01, min_lift=1.00, min_conviction=1.00, max_len=2, start_pos=21563, end_pos=25385, start_af=0.20, end_af=1.00, funclass=[\"MISSENSE\"])\n",
        "num_rules = bos_rules.shape[0]\n",
        "print('Boston dataset association rules: ')\n",
        "bos_rules = add_readable_rules(bos_rules)\n",
        "bos_rules = add_readable_rules_translated(bos_rules, 21563)\n",
        "print(bos_rules.head(num_rules))\n",
        "print('\\n\\n')\n",
        "# bos_rules.to_csv('/content/gdrive/MyDrive/Colab Notebooks/MBA_files/bos_0223_0905_2863_7000.csv', sep=',')\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Boston dataset association rules: \n",
            "   antecedents consequents                                       translated_readable_rule                                                     readable_rule  antecedent support  consequent support   support  confidence       lift  leverage  conviction\n",
            "0    (2157533)   (2340333)     ((5, Misense, AF >= 0.80)) => ((614, Misense, AF >= 0.80))  ((21575, Misense, AF >= 0.80)) => ((23403, Misense, AF >= 0.80))            0.013289            0.996678  0.013289    1.000000   1.003333  0.000044         inf\n",
            "1    (2340333)   (2157533)     ((614, Misense, AF >= 0.80)) => ((5, Misense, AF >= 0.80))  ((23403, Misense, AF >= 0.80)) => ((21575, Misense, AF >= 0.80))            0.996678            0.013289  0.013289    0.013333   1.003333  0.000044    1.000045\n",
            "2    (2160433)   (2340333)    ((14, Misense, AF >= 0.80)) => ((614, Misense, AF >= 0.80))  ((21604, Misense, AF >= 0.80)) => ((23403, Misense, AF >= 0.80))            0.013289            0.996678  0.013289    1.000000   1.003333  0.000044         inf\n",
            "3    (2340333)   (2160433)    ((614, Misense, AF >= 0.80)) => ((14, Misense, AF >= 0.80))  ((23403, Misense, AF >= 0.80)) => ((21604, Misense, AF >= 0.80))            0.996678            0.013289  0.013289    0.013333   1.003333  0.000044    1.000045\n",
            "4    (2165633)   (2199333)    ((32, Misense, AF >= 0.80)) => ((144, Misense, AF >= 0.80))  ((21656, Misense, AF >= 0.80)) => ((21993, Misense, AF >= 0.80))            0.013289            0.008306  0.008306    0.625000  75.250000  0.008195    2.644518\n",
            "5    (2199333)   (2165633)    ((144, Misense, AF >= 0.80)) => ((32, Misense, AF >= 0.80))  ((21993, Misense, AF >= 0.80)) => ((21656, Misense, AF >= 0.80))            0.008306            0.013289  0.008306    1.000000  75.250000  0.008195         inf\n",
            "6    (2165633)   (2340333)    ((32, Misense, AF >= 0.80)) => ((614, Misense, AF >= 0.80))  ((21656, Misense, AF >= 0.80)) => ((23403, Misense, AF >= 0.80))            0.013289            0.996678  0.013289    1.000000   1.003333  0.000044         inf\n",
            "7    (2340333)   (2165633)    ((614, Misense, AF >= 0.80)) => ((32, Misense, AF >= 0.80))  ((23403, Misense, AF >= 0.80)) => ((21656, Misense, AF >= 0.80))            0.996678            0.013289  0.013289    0.013333   1.003333  0.000044    1.000045\n",
            "8    (2165633)   (2524433)   ((32, Misense, AF >= 0.80)) => ((1228, Misense, AF >= 0.80))  ((21656, Misense, AF >= 0.80)) => ((25244, Misense, AF >= 0.80))            0.013289            0.013289  0.013289    1.000000  75.250000  0.013112         inf\n",
            "9    (2524433)   (2165633)   ((1228, Misense, AF >= 0.80)) => ((32, Misense, AF >= 0.80))  ((25244, Misense, AF >= 0.80)) => ((21656, Misense, AF >= 0.80))            0.013289            0.013289  0.013289    1.000000  75.250000  0.013112         inf\n",
            "10   (2199333)   (2340333)   ((144, Misense, AF >= 0.80)) => ((614, Misense, AF >= 0.80))  ((21993, Misense, AF >= 0.80)) => ((23403, Misense, AF >= 0.80))            0.008306            0.996678  0.008306    1.000000   1.003333  0.000028         inf\n",
            "12   (2524433)   (2199333)  ((1228, Misense, AF >= 0.80)) => ((144, Misense, AF >= 0.80))  ((25244, Misense, AF >= 0.80)) => ((21993, Misense, AF >= 0.80))            0.013289            0.008306  0.008306    0.625000  75.250000  0.008195    2.644518\n",
            "13   (2199333)   (2524433)  ((144, Misense, AF >= 0.80)) => ((1228, Misense, AF >= 0.80))  ((21993, Misense, AF >= 0.80)) => ((25244, Misense, AF >= 0.80))            0.008306            0.013289  0.008306    1.000000  75.250000  0.008195         inf\n",
            "14   (2200733)   (2340333)   ((149, Misense, AF >= 0.80)) => ((614, Misense, AF >= 0.80))  ((22007, Misense, AF >= 0.80)) => ((23403, Misense, AF >= 0.80))            0.014950            0.996678  0.014950    1.000000   1.003333  0.000050         inf\n",
            "15   (2340333)   (2200733)   ((614, Misense, AF >= 0.80)) => ((149, Misense, AF >= 0.80))  ((23403, Misense, AF >= 0.80)) => ((22007, Misense, AF >= 0.80))            0.996678            0.014950  0.014950    0.015000   1.003333  0.000050    1.000051\n",
            "16   (2409633)   (2340333)   ((845, Misense, AF >= 0.80)) => ((614, Misense, AF >= 0.80))  ((24096, Misense, AF >= 0.80)) => ((23403, Misense, AF >= 0.80))            0.079734            0.996678  0.079734    1.000000   1.003333  0.000265         inf\n",
            "17   (2340333)   (2409633)   ((614, Misense, AF >= 0.80)) => ((845, Misense, AF >= 0.80))  ((23403, Misense, AF >= 0.80)) => ((24096, Misense, AF >= 0.80))            0.996678            0.079734  0.079734    0.080000   1.003333  0.000265    1.000289\n",
            "18   (2524433)   (2340333)  ((1228, Misense, AF >= 0.80)) => ((614, Misense, AF >= 0.80))  ((25244, Misense, AF >= 0.80)) => ((23403, Misense, AF >= 0.80))            0.013289            0.996678  0.013289    1.000000   1.003333  0.000044         inf\n",
            "19   (2340333)   (2524433)  ((614, Misense, AF >= 0.80)) => ((1228, Misense, AF >= 0.80))  ((23403, Misense, AF >= 0.80)) => ((25244, Misense, AF >= 0.80))            0.996678            0.013289  0.013289    0.013333   1.003333  0.000044    1.000045\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5erF_k0uKLI"
      },
      "source": [
        "# Anton\n",
        "pd.set_option('max_columns', 10, 'display.expand_frame_repr', False)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "bos_rules = get_association_rules(in_file=\"https://github.com/galaxyproject/SARS-CoV-2/raw/master/data/var/bos_by_sample.tsv.gz\", min_support=0.050, min_confidence=0.800, min_lift=1.0, min_conviction=1.0, max_len=6, end_af=0.80, start_prob=0.000, end_prob=0.250, funclass=[\"MISSENSE\", \"SILENT\"])\n",
        "num_rules = bos_rules.shape[0]\n",
        "print('Number of rules: {}'.format(num_rules))\n",
        "print('Boston dataset association rules: ')\n",
        "bos_rules = add_readable_rules(bos_rules)\n",
        "print(bos_rules.head(num_rules))\n",
        "print('\\n\\n')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEa5yn9JFJDI"
      },
      "source": [
        "# Test get_position_probability()\n",
        "\n",
        "pd.set_option('max_columns', 10, 'display.expand_frame_repr', False)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# Read the input file and pick the needed columns\n",
        "in_file = \"https://github.com/galaxyproject/SARS-CoV-2/raw/master/data/var/bos_by_sample.tsv.gz\"\n",
        "df_in = pd.read_csv(in_file, sep='\\t')[['Sample', 'POS', 'AF', 'FUNCLASS']]\n",
        "\n",
        "position=26542\n",
        "position_prob = get_position_probability(df_in, position=position)\n",
        "print(\"position: {}, position_prob: {:.4f}\".format(position, position_prob))\n",
        "\n",
        "position=26542\n",
        "position_prob = get_position_probability(df_in, position=position)\n",
        "print(\"position: {}, position_prob: {:.4f}\".format(position, position_prob))\n",
        "\n",
        "position=23403\n",
        "position_prob = get_position_probability(df_in, position=position)\n",
        "print(\"position: {}, position_prob: {:.4f}\".format(position, position_prob))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wf7flupelghL",
        "outputId": "9f943bcd-718a-467b-9ae6-0ee6c78bae64"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from mlxtend.frequent_patterns import apriori, association_rules\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "\n",
        "pd.set_option('max_columns', 10, 'display.expand_frame_repr', False)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "in_file = \"/content/gdrive/MyDrive/Colab Notebooks/MBA_files/mba_input.csv\"\n",
        "out_file = \"/content/gdrive/MyDrive/Colab Notebooks/MBA_files/mba_output.csv\"\n",
        "max_len = 4\n",
        "min_support=0.6\n",
        "min_confidence=0.6\n",
        "min_lift=1.0\n",
        "min_conviction=1.0\n",
        "\n",
        "with open(in_file) as fp:\n",
        "    lines = fp.read().splitlines() \n",
        "\n",
        "dataset = []\n",
        "for line in lines:\n",
        "  line_items = line.split(\",\")\n",
        "  dataset.append(line_items)\n",
        "  \n",
        "te = TransactionEncoder()\n",
        "te_ary = te.fit_transform(dataset)\n",
        "\n",
        "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
        "\n",
        "frequent_itemsets = apriori(df, min_support=min_support, use_colnames=True, max_len=max_len)\n",
        "\n",
        "# Get association rules, with lift larger than min_lift  \n",
        "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=min_confidence)\n",
        "\n",
        "# Filter association rules, keeping rules with lift and conviction larger than min_liftand min_conviction\n",
        "rules = rules[ (rules['lift'] >= min_lift) & (rules['conviction'] >= min_conviction) ]\n",
        "\n",
        "rules['antecedents'] = rules['antecedents'].apply(list)\n",
        "rules['consequents'] = rules['consequents'].apply(list)\n",
        "\n",
        "rules.to_csv(out_file)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                  antecedents                consequents  antecedent support  consequent support  support  confidence  lift  leverage  conviction\n",
            "0            ('Kidney Beans')                   ('Eggs')                 1.0                 0.8      0.8        0.80  1.00      0.00         1.0\n",
            "1                    ('Eggs')           ('Kidney Beans')                 0.8                 1.0      0.8        1.00  1.00      0.00         inf\n",
            "2                   ('Onion')                   ('Eggs')                 0.6                 0.8      0.6        1.00  1.25      0.12         inf\n",
            "3                    ('Eggs')                  ('Onion')                 0.8                 0.6      0.6        0.75  1.25      0.12         1.6\n",
            "4            ('Kidney Beans')                   ('Milk')                 1.0                 0.6      0.6        0.60  1.00      0.00         1.0\n",
            "5                    ('Milk')           ('Kidney Beans')                 0.6                 1.0      0.6        1.00  1.00      0.00         inf\n",
            "6                   ('Onion')           ('Kidney Beans')                 0.6                 1.0      0.6        1.00  1.00      0.00         inf\n",
            "7            ('Kidney Beans')                  ('Onion')                 1.0                 0.6      0.6        0.60  1.00      0.00         1.0\n",
            "8                  ('Yogurt')           ('Kidney Beans')                 0.6                 1.0      0.6        1.00  1.00      0.00         inf\n",
            "9            ('Kidney Beans')                 ('Yogurt')                 1.0                 0.6      0.6        0.60  1.00      0.00         1.0\n",
            "10  ('Onion', 'Kidney Beans')                   ('Eggs')                 0.6                 0.8      0.6        1.00  1.25      0.12         inf\n",
            "11   ('Eggs', 'Kidney Beans')                  ('Onion')                 0.8                 0.6      0.6        0.75  1.25      0.12         1.6\n",
            "12          ('Onion', 'Eggs')           ('Kidney Beans')                 0.6                 1.0      0.6        1.00  1.00      0.00         inf\n",
            "13           ('Kidney Beans')          ('Onion', 'Eggs')                 1.0                 0.6      0.6        0.60  1.00      0.00         1.0\n",
            "14                  ('Onion')   ('Eggs', 'Kidney Beans')                 0.6                 0.8      0.6        1.00  1.25      0.12         inf\n",
            "15                   ('Eggs')  ('Onion', 'Kidney Beans')                 0.8                 0.6      0.6        0.75  1.25      0.12         1.6\n",
            "                  antecedents                consequents  antecedent support  consequent support  support  confidence  lift  leverage  conviction\n",
            "0            ['Kidney Beans']                   ['Eggs']                 1.0                 0.8      0.8        0.80  1.00      0.00         1.0\n",
            "1                    ['Eggs']           ['Kidney Beans']                 0.8                 1.0      0.8        1.00  1.00      0.00         inf\n",
            "2                   ['Onion']                   ['Eggs']                 0.6                 0.8      0.6        1.00  1.25      0.12         inf\n",
            "3                    ['Eggs']                  ['Onion']                 0.8                 0.6      0.6        0.75  1.25      0.12         1.6\n",
            "4            ['Kidney Beans']                   ['Milk']                 1.0                 0.6      0.6        0.60  1.00      0.00         1.0\n",
            "5                    ['Milk']           ['Kidney Beans']                 0.6                 1.0      0.6        1.00  1.00      0.00         inf\n",
            "6                   ['Onion']           ['Kidney Beans']                 0.6                 1.0      0.6        1.00  1.00      0.00         inf\n",
            "7            ['Kidney Beans']                  ['Onion']                 1.0                 0.6      0.6        0.60  1.00      0.00         1.0\n",
            "8                  ['Yogurt']           ['Kidney Beans']                 0.6                 1.0      0.6        1.00  1.00      0.00         inf\n",
            "9            ['Kidney Beans']                 ['Yogurt']                 1.0                 0.6      0.6        0.60  1.00      0.00         1.0\n",
            "10  ['Onion', 'Kidney Beans']                   ['Eggs']                 0.6                 0.8      0.6        1.00  1.25      0.12         inf\n",
            "11   ['Eggs', 'Kidney Beans']                  ['Onion']                 0.8                 0.6      0.6        0.75  1.25      0.12         1.6\n",
            "12          ['Onion', 'Eggs']           ['Kidney Beans']                 0.6                 1.0      0.6        1.00  1.00      0.00         inf\n",
            "13           ['Kidney Beans']          ['Onion', 'Eggs']                 1.0                 0.6      0.6        0.60  1.00      0.00         1.0\n",
            "14                  ['Onion']   ['Eggs', 'Kidney Beans']                 0.6                 0.8      0.6        1.00  1.25      0.12         inf\n",
            "15                   ['Eggs']  ['Onion', 'Kidney Beans']                 0.8                 0.6      0.6        0.75  1.25      0.12         1.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIdo8vJ03XPm"
      },
      "source": [
        "\n",
        "dataset = [['Milk', 'Onion', 'Nutmeg', 'Kidney Beans', 'Eggs', 'Yogurt'],\n",
        "           ['Dill', 'Onion', 'Nutmeg', 'Kidney Beans', 'Eggs', 'Yogurt'],\n",
        "           ['Milk', 'Apple', 'Kidney Beans', 'Eggs'],\n",
        "           ['Milk', 'Unicorn', 'Corn', 'Kidney Beans', 'Yogurt'],\n",
        "           ['Corn', 'Onion', 'Onion', 'Kidney Beans', 'Ice cream', 'Eggs']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qgvMjke3XG-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3ASKXDBeOsr"
      },
      "source": [
        "---\n",
        "**MBA parameter**s \n",
        "\n",
        "1.   **bos_rules**: *support*=0.223, *confidence*=0.905, *lift*=2.863, *conviction*=7.000\n",
        "2.   **uke_rules**: *support*=0.210, *confidence*=0.910, *lift*=2.500, conviction *italicized text*=7.000\n",
        "3.   **ukl_rules**: *support*=0.203, *confidence*=0.926, *lift*=2.030, *conviction*=7.000\n",
        "---\n",
        "The last digit of an entry is Allele Frequency (**AF**)\n",
        "\n",
        "*   **3**: $>=$ 0.80    \n",
        "*   **2**: $>=$ 0.20 and $<$ 0.80\n",
        "*   **1**: $<$ 0.20\n",
        "---\n",
        "The digit before the last is **Funclass**\n",
        "\n",
        "*  **4**: None\n",
        "*  **3**: Missense\n",
        "*  **2**: Nonsense\n",
        "*  **1**: Silent\n",
        "---\n",
        "**Boston association rules** (12 rules)\n",
        "\n",
        "**Antecedent** entries\n",
        "\n",
        "* 3037**13**  (5 times)\n",
        "* 14408**33** (5 times)\n",
        "* 23403**33** (5 times)\n",
        "* 26542**31** (All 12)\n",
        "\n",
        "**Consequent** entries\n",
        "\n",
        "* 3037**13** (5 times)\n",
        "* 14408**33** (5 times)\n",
        "* 23403**33** (5 times)\n",
        "* 26545**31** (All 12)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XszXqioqokmE"
      },
      "source": [
        "---\n",
        "**UK early association rules** (10 rules)\n",
        "\n",
        "**Antecedent** entries\n",
        "\n",
        "* 241**43**   (9 times)\n",
        "* 14408**33** (3 times)\n",
        "* 23403**33** (4 times)\n",
        "* 28881**32** (All 10)\n",
        "\n",
        "**Consequent** entries\n",
        "\n",
        "* 3037**12**  (All 10)\n",
        "* 14408**33** (3 times)\n",
        "* 23403**33** (3 times)\n",
        "* 28883**33** (All 10)\n",
        "\n",
        "---\n",
        "**UK late association rules** (11 rules)\n",
        "\n",
        "**Antecedent** entries\n",
        "\n",
        "* 204**42** (All 11)\n",
        "* 445**13** (All 11)\n",
        "* 6286**13** (3 times)\n",
        "* 21255**13** (All 11)\n",
        "* 23403**33** (3 times)\n",
        "* 28932**32** (1 time)\n",
        "\n",
        "**Consequent** entries\n",
        "\n",
        "* 6286**13** (5 times)\n",
        "* 22227**32** (All 11)\n",
        "* 23403**33** (5 times)\n",
        "* 27944**13** (10 times)\n",
        "* 28932**32** (9 time)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTP92PqMymr8"
      },
      "source": [
        "**Entries that show up in antecedent/consequent across samples**\n",
        "\n",
        "**Antecedent**\n",
        "\n",
        "* 14408**33** (Boston, UKE)\n",
        "* 23403**33** (Boston, UKE, UKL)\n",
        "\n",
        "**Consequent**\n",
        "\n",
        "* 3037**13** (Boston, UKE)\n",
        "* 14408**33** (Boston, UKE)\n",
        "* 23403**33** (Boston, UKE, UKL)\n",
        "\n"
      ]
    }
  ]
}