{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MBA_new_data",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kxk302/MBA/blob/new_data/MBA_new_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JDt_5RMdjL7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b98b43c-faf9-4fb6-f581-e4dbf574b9c0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0bkibi4dy-V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bf37a49-f1bb-4dc8-b87e-e112262ca341"
      },
      "source": [
        "!ls '/content/gdrive/MyDrive/Colab Notebooks/'"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Antecedents_2.png  Consequent_2.png  HIV\t    MBA_new_data.ipynb\n",
            "Antecedents_3.png  Consequent_3.png  hiv_rules.csv\n",
            "Antecedents_4.png  Consequent_4.png  MBA.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8BTJgr3VIiP"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from mlxtend.frequent_patterns import apriori\n",
        "from mlxtend.frequent_patterns import association_rules\n",
        "\n",
        "af_dict = {\n",
        "  \"1\":\"AF < 0.20\",\n",
        "  \"2\": \"0.80 > AF >= 0.20\",\n",
        "  \"3\":\"AF >= 0.80\"\n",
        "}\n",
        "\n",
        "effect_dict = {\n",
        "  \"1\": \"NON_SYNONYMOUS_CODING\",\n",
        "  \"2\": \"SYNONYMOUS_CODING\",\n",
        "  \"3\": \"CODON_CHANGE_PLUS_CODON_DELETION\",\n",
        "  \"4\": \"CODON_DELETION\",\n",
        "  \"5\": \"STOP_GAINED\",\n",
        "  \"6\": \"FRAME_SHIFT\"\n",
        "}\n",
        "\n",
        "def to_str_each(items):\n",
        "  items_str = \"\"\n",
        "  for item in items:\n",
        "    # Label = EFFECT + AF + POS\n",
        "    item_str = \"(\" + effect_dict.get(str(item)[0]) + \", \" + af_dict.get(str(item)[1]) + \", \" + str(item)[2:] + \") & \"\n",
        "    items_str += item_str\n",
        "  return items_str[:-3]\n",
        "\n",
        "def convert_str_list_to_int_list(str_list):\n",
        "  return [int(elem) for elem in str_list]\n",
        "\n",
        "# Convert series of antecedents/consequents to human readable \n",
        "def to_str(ser):    \n",
        "  # Convert forzenset to list\n",
        "  ser = ser.apply(lambda x: list(x))\n",
        "\n",
        "  # Convert list elements from string to int\n",
        "  ser = ser.apply(convert_str_list_to_int_list)\n",
        "\n",
        "  return ser.apply(to_str_each)\n",
        "\n",
        "def add_readable_rules(df_in):\n",
        "  # Empty data frame\n",
        "  if df_in is None or df_in.shape[0] == 0:\n",
        "    return df_in\n",
        "\n",
        "  df = df_in.copy()\n",
        "\n",
        "  antecedents_str = to_str(df['antecedents'])\n",
        "  consequents_str = to_str(df['consequents'])\n",
        "\n",
        "  readable_rule = \"(\" + antecedents_str + \") => (\" + consequents_str + \")\"\n",
        "  df.insert(2,'readable_rule', readable_rule)\n",
        "\n",
        "  return df\n",
        "\n",
        "def filter_on_position_probability(df_in, start_prob=None, end_prob=None):\n",
        "    if df_in is None or df_in.shape[0] == 0:\n",
        "      return df_in\n",
        "\n",
        "    if start_prob is None and end_prob is None:\n",
        "      return df_in\n",
        "\n",
        "    # Only consider rows where probability of Position being present in the Samples is >= start_prob and <= end_prob\n",
        "    num_samples = df_in['Sample'].nunique()\n",
        "    print(\"num_samples: {}\".format(num_samples))\n",
        "\n",
        "    num_positions = df_in['POS'].nunique()\n",
        "    print(\"num_positions: {}\".format(num_positions))\n",
        "   \n",
        "    df = df_in[['Sample', 'POS']].groupby(['POS']).agg(position_prob=('Sample', 'count'))\n",
        "    # Normalize position_prob by dividing it by num_samples. This makes it a proability between 0.0 and 1.0\n",
        "    df['position_prob'] = df['position_prob'] / num_samples   \n",
        "\n",
        "    df_sorted = df.sort_values(by = 'position_prob')\n",
        "    print(df_sorted.head())\n",
        "    print(df_sorted.tail())\n",
        "  \n",
        "    if start_prob is not None:\n",
        "      df = df[df.position_prob >= start_prob]\n",
        "\n",
        "    if end_prob is not None:\n",
        "      df = df[df.position_prob <= end_prob]\n",
        "    \n",
        "    ret_val = pd.merge(df_in, df, on='POS')\n",
        "\n",
        "    df_sorted = ret_val.sort_values(by = 'position_prob')[['POS', 'position_prob']]\n",
        "    print(df_sorted.head())\n",
        "    print(df_sorted.tail())\n",
        "\n",
        "    return ret_val\n",
        "\n",
        "def get_position_probability(df_in, position=None):\n",
        "    if df_in is None or df_in.shape[0] == 0:\n",
        "      return df_in\n",
        "\n",
        "    if position is None:\n",
        "      return df_in\n",
        "\n",
        "    num_samples = df_in['Sample'].nunique()\n",
        "    print(\"num_samples: {}\".format(num_samples))\n",
        "\n",
        "    num_positions = df_in['POS'].nunique()\n",
        "    print(\"num_positions: {}\".format(num_positions))\n",
        "   \n",
        "    df = df_in[['Sample', 'POS']].groupby(['POS']).agg(position_prob=('Sample', 'count'))\n",
        "    # Normalize position_prob by dividing it by num_samples. This makes it a proability between 0.0 and 1.0\n",
        "    df['position_prob'] = df['position_prob'] / num_samples\n",
        "\n",
        "    try:      \n",
        "      return df.loc[position,'position_prob']\n",
        "    except KeyError:\n",
        "      return -1\n",
        "\n",
        "def filter_on_column_values(df_in, start_pos=None, end_pos=None, start_af=None, end_af=None, effect=[]):  \n",
        "  if df_in is None or df_in.shape[0] == 0:\n",
        "    return df_in\n",
        "\n",
        "  df = df_in.copy()\n",
        "\n",
        "  # Replace \".\" with \"NONE\" in FUNCLASS column. They both represent \"Non-coding\" variant\n",
        "  df[\"EFFECT\"] = df[\"EFFECT\"].replace('.', 'NONE')\n",
        "\n",
        "  # Filter in_file rows\n",
        "  if start_pos is not None:\n",
        "    df = df[df.POS >= start_pos]    \n",
        "  if end_pos is not None:\n",
        "    df = df[df.POS < end_pos]    \n",
        "  if start_af is not None:\n",
        "    df = df[df.AF >= start_af]    \n",
        "  if end_af is not None:\n",
        "    df = df[df.AF < end_af]    \n",
        "  if len(effect) > 0:    \n",
        "    df = df[df.EFFECT.isin(effect)]\n",
        "\n",
        "  return df\n",
        "\n",
        "# Create a single integer representing a variant at a specific position with a specific allele frequency\n",
        "# Pivot the data so we have all sample variants on a single line\n",
        "#\n",
        "# Extract rows from in_file where\n",
        "# \n",
        "#    POS >= start_pos & POS <= end_pos\n",
        "#        If start_pos = None: POS <= end_pos\n",
        "#        If end_pos = None: POS >= start_pos \n",
        "#\n",
        "#    AF >= start_af & AF <= end_af\n",
        "#        If start_af = None: AF <= end_af\n",
        "#        If end_af = None: AF >= start_af\n",
        "#\n",
        "#    EFFECT in effect (effect is a dict)      \n",
        "#\n",
        "def preprocess_input_file(df_in, effect):\n",
        "  if df_in is None or df_in.shape[0] == 0:\n",
        "    return df_in\n",
        "  \n",
        "  df = df_in.copy()\n",
        "\n",
        "  # Bucket values in EFFECT and AF columns. We do not bucket the values in POS column.\n",
        "\n",
        "  # Replace variants in EFFECT column with a distinct numeric value\n",
        "  for key, value in effect.items():\n",
        "    df.loc[df.EFFECT == key, \"EFFECT\"] = value \n",
        "  \n",
        "  # Replace allele frequency in AF column with a distict numeric value\n",
        "  df.loc[df.AF >= 0.80, \"AF\"] = 3\n",
        "  df.loc[(df.AF >= 0.20) & (df.AF < 0.80), \"AF\"] = 2\n",
        "  df.loc[df.AF < 0.20, \"AF\"] = 1\n",
        "\n",
        "  # Convert AF values to integer\n",
        "  df = df.astype({\"AF\": int}) \n",
        "\n",
        "  # Get the largest position value\n",
        "  max_pos = df[\"POS\"].max()\n",
        "  # Get the length of max_pos string\n",
        "  max_pos_len = len(str(max_pos))\n",
        "\n",
        "  # Create a new column called 'Label', which is a string concatentation of POS, EFFECT, and AF values. \n",
        "  # The idea is to represent each variant + allele frequency + position as a single integer, to be used in MBA \n",
        "  # zero fill position, so all labels have the same length\n",
        "  df[\"POS\"].apply(lambda x: str(x).zfill(max_pos_len))\n",
        "  df[\"Label\"] = + df[\"EFFECT\"].astype(str) + df[\"AF\"].astype(str) + df[\"POS\"].astype(str)\n",
        "\n",
        "  # We do not need POS, EFFECT, and AF columns anymore\n",
        "  df = df[[\"Sample\", \"Label\"]]\n",
        "  \n",
        "  # Add a new column called 'Value', prepopulated with 1\n",
        "  df[\"Value\"] = 1\n",
        "\n",
        "  df = pd.pivot_table(df, index=\"Sample\", columns=\"Label\", values=\"Value\")\n",
        "\n",
        "  # Set all data frame nan (not a number) values to 0\n",
        "  df = df.fillna(0)\n",
        "\n",
        "  # Convert all data framevalues to integer\n",
        "  df = df.astype(int) \n",
        "\n",
        "  return df"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VubP1ctVUsc"
      },
      "source": [
        "def get_association_rules(in_file, min_support=0.20, min_confidence=0.80, min_lift=1.0, min_conviction=1.0, max_len=None, start_pos=None, end_pos=None, start_af=None, end_af=None, effect={}, start_prob=None, end_prob=None):\n",
        "  # Read the input file and pick the needed columns\n",
        "  df_in = pd.read_csv(in_file, sep='\\t', encoding='ISO-8859â€“1')[['Sample', 'POS', 'AF', 'EFFECT']]\n",
        "\n",
        "  # Filter on position probability\n",
        "  df_pp = filter_on_position_probability(df_in, start_prob, end_prob)\n",
        "\n",
        "  # Filter on column values\n",
        "  df_cv = filter_on_column_values(df_pp, start_pos, end_pos, start_af, end_af, effect.keys())\n",
        "\n",
        "  # Preprocess the data frame\n",
        "  df = preprocess_input_file(df_cv, effect)\n",
        "\n",
        "  # Get frequent item sets, with support larger than min_support, using Apriori algorithm\n",
        "  frequent_itemsets = apriori(df, min_support=min_support, use_colnames=True, max_len=max_len)\n",
        "\n",
        "  # Get association rules, with lift larger than min_lift  \n",
        "  rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=min_lift)\n",
        "\n",
        "  # Filter association rules, keeping rules with confidence larger than min_confidence\n",
        "  rules = rules[ (rules['confidence'] >= min_confidence) & (rules['conviction'] >= min_conviction) ]\n",
        "\n",
        "  return rules"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5erF_k0uKLI"
      },
      "source": [
        "pd.set_option('max_columns', 10, 'display.expand_frame_repr', False)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "effect = {\n",
        "    \"NON_SYNONYMOUS_CODING\": 1, \n",
        "    \"SYNONYMOUS_CODING\": 2,\n",
        "    \"CODON_CHANGE_PLUS_CODON_DELETION\": 3,\n",
        "    \"CODON_DELETION\": 4,\n",
        "    \"STOP_GAINED\": 5,\n",
        "    \"FRAME_SHIFT\": 6\n",
        "}\n",
        "\n",
        "rules = get_association_rules(in_file=\"https://github.com/kxk302/MBA/raw/new_data/data/gx-observable_data_PRJEB44141_decomp.tsv\",\n",
        "                              min_support=0.250, \n",
        "                              min_confidence=0.800, \n",
        "                              min_lift=2.0, \n",
        "                              min_conviction=2.0, \n",
        "                              max_len=3, \n",
        "                              effect=effect)\n",
        "\n",
        "num_rules = rules.shape[0]\n",
        "print('Number of rules: {}'.format(num_rules))\n",
        "print('Dataset association rules: ')\n",
        "rules = add_readable_rules(rules)\n",
        "rules.sort_values(by=[\"conviction\"], ascending=False, inplace=True)\n",
        "print(rules.head(num_rules))\n",
        "print('\\n\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayq0VRKx78rQ"
      },
      "source": [
        "**Columns in gx-observable_data_PRJEB44141_decomp.tsv.gz (118593 rows including header)**\n",
        "\n",
        "Sample: Sample ID \\\\\n",
        "POS: Position \\\\\n",
        "REF: Reference allele. Base that's found in the SARS-CoV-2 reference genome from NCBI \\\\\n",
        "ALT: Alternate allele. Something other than REF \\\\\n",
        "EFFECT: Type of variation (See text box below) \\\\\n",
        "CODON: Triplet changes \\\\\n",
        "TRID: Transcript identifier. The name of the gene product that is affected by the variant \\\\\n",
        "AA: Amino acid \\\\\n",
        "AF: Allele frequency. At the variant site what fraction of spanning reads supports the ALT allele \\\\"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-Fu3CY-uLj8"
      },
      "source": [
        "**EFFECT possible values (22 in total)**\n",
        "\n",
        ". What is dot? \\\\\n",
        "\n",
        "**CODON_CHANGE_PLUS_CODON_DELETION**  \\\\\n",
        "E.g., GAGTTCA -> G, gagttcaga/gga.\t**EFR156G**: AA EFR at position 156 changed to G \\\\\n",
        "OR, \\\\\n",
        "E.g., TTTA -> T, tattac/tac.\t**YY144Y**: AA YY at position 144 changed to AA Y\n",
        "\n",
        "**CODON_CHANGE_PLUS_CODON_DELETION+SPLICE_SITE_REGION** \\\\\n",
        "E.g., TTGTTAACAA -> T, cttgttaacaac/ctc. **LVNN295L**: AA LVNN at position 298 changes to AA L \\\\\n",
        "\n",
        "**CODON_CHANGE_PLUS_CODON_INSERTION** \\\\\n",
        " E.g., T -> TGCC, gta/gtGGCa. V258VA: AA V at position 258 changes to AA VA \\\\\n",
        "\n",
        "**CODON_DELETION** \\\\\n",
        "E.g., AGATTTC\t-> A, gatttc/\t. **DF119**: AA DF at position 119 are deleted \\\\\n",
        "\n",
        "**CODON_INSERTION** \\\\\n",
        "E.g., T\t-> TGGC,\tgta/gtGGCa. **V258VA**: AA V at position 258 changes to AA VA \\\\\n",
        "\n",
        "**FRAME_SHIFT**  \\\\\n",
        "E.g., TCA\t-> T,\taca/ . **T148**: AA T at position 148 is deleted \\\\\n",
        "\n",
        "**FRAME_SHIFT+SPLICE_SITE_REGION** \\\\\n",
        "E.g., CA -> C, caa/ . **Q346**: AA Q at position 346 is deleted \\\\\n",
        "\n",
        "**FRAME_SHIFT+START_LOST** \\\\\n",
        "E.g., ATG\t-> A,\tatg/ .\t**M1**: AA M at position 1 is deleted \\\\\n",
        "\n",
        "**FRAME_SHIFT+STOP_GAINED** \\\\\n",
        "E.g., C -> CT, gat/Tgat. **D124*?**: AA D at position 124 changes to stop codon \\\\\n",
        "\n",
        "**FRAME_SHIFT+STOP_LOST+SPLICE_SITE_REGION** \\\\\n",
        "E.g., AGCAATCTTT -> A,\ttag/ . \t***39**: Stop codon in position 39 deleted ? \\\\\n",
        "\n",
        "**GENE_FUSION** \\\\\n",
        "E.g., AAACGAACATGAAATT -> A. \tNo AA specified ? \\\\\n",
        "\n",
        "**NON_SYNONYMOUS_CODING** \\\\\n",
        "E.g., G\t-> T,\tGat/Tat. \t**D377Y**: AA D at position 377 changes to AA Y \\\\\n",
        "\n",
        "**NON_SYNONYMOUS_CODING+SPLICE_SITE_REGION** \\\\\n",
        "E.g., G -> T,\tcaG/caT.\t**Q290H**: AA Q at position 290 changes to AA H \\\\\n",
        "\n",
        "**SPLICE_SITE_REGION+SYNONYMOUS_CODING** \\\\\n",
        "E.g., C -> T,\taaC/aaT. **N298** : AA N at position 298 does NOT change ? \\\\\n",
        "\n",
        "**SPLICE_SITE_REGION+SYNONYMOUS_STOP** \\\\\n",
        "E.g., A\t-> G,\ttAa/tGa, \t***122**: Stop codon at position 122 is deleted \\\\\n",
        "\n",
        "**START_LOST** \\\\\n",
        "E.g., G -> A,\tatG/atA. **M1I**: AA M at position 1 changes to AA I. \\\\\n",
        "\n",
        "**STOP_GAINED** \\\\\n",
        "E.g., A -> T,\tAaa/Taa.\t**K68***: AA K at position 68 changes into a stop codon \\\\\n",
        "\n",
        "**STOP_GAINED+CODON_CHANGE_PLUS_CODON_DELETION** \\\\\n",
        "E.g., TATA -> T,\ttataaa/taa.\t**YK1064***: AA YK at position 1064 changes to stop codon \\\\\n",
        "\n",
        "**STOP_GAINED+CODON_INSERTION** \\\\\n",
        "E.g., T -> TAATCTCACATAGCAATCTTTAATC,\tcaa/CAATCTTTAATCAATCTCACATAGcaa.\t**Q29QSLINLT*Q**: AA Q at position 29 changes to AA QSLINLT*Q, with * being the stop codon ? \\\\\n",
        "\n",
        "**STOP_LOST+SPLICE_SITE_REGION** \\\\\n",
        "E.g., GATT -> G,\ttga/ .\t***122**: stop codon at position 122 is deleted \\\\\n",
        "\n",
        "**SYNONYMOUS_CODING** \\\\\n",
        "E.g., C -> T, Cta/Tta. **L387**: AA L at position 387 is unchanged \\\\\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkIbJ8D_SFa8"
      },
      "source": [
        "**EFFECT counts in gx-observable_data_PRJEB44141_decomp.tsv.gz**\n",
        "\n",
        ".: 8507 \\\\\n",
        "**CODON_CHANGE_PLUS_CODON_DELETION: 4118** \\\\\n",
        "CODON_CHANGE_PLUS_CODON_DELETION+SPLICE_SITE_REGION: 2 \\\\\n",
        "CODON_CHANGE_PLUS_CODON_INSERTION: 7 \\\\\n",
        "**CODON_DELETION: 2724** \\\\\n",
        "CODON_INSERTION: 46 \\\\\n",
        "**FRAME_SHIFT: 1232** \\\\\n",
        "FRAME_SHIFT+SPLICE_SITE_REGION: 1 \\\\\n",
        "FRAME_SHIFT+START_LOST: 3 \\\\\n",
        "FRAME_SHIFT+STOP_GAINED: 197 \\\\\n",
        "FRAME_SHIFT+STOP_LOST+SPLICE_SITE_REGION: 3 \\\\\n",
        "GENE_FUSION: 47 \\\\\n",
        "**NON_SYNONYMOUS_CODING: 70726** \\\\\n",
        "NON_SYNONYMOUS_CODING+SPLICE_SITE_REGION: 6 \\\\\n",
        "SPLICE_SITE_REGION+SYNONYMOUS_CODING: 1 \\\\\n",
        "SPLICE_SITE_REGION+SYNONYMOUS_STOP: 5 \\\\\n",
        "START_LOST: 7 \\\\\n",
        "**STOP_GAINED: 2241** \\\\\n",
        "STOP_GAINED+CODON_CHANGE_PLUS_CODON_DELETION: 3 \\\\\n",
        "STOP_GAINED+CODON_INSERTION: 2 \\\\\n",
        "STOP_LOST+SPLICE_SITE_REGION: 1 \\\\\n",
        "**SYNONYMOUS_CODING: 28713** \\\\\n",
        "\n",
        "Total: 118582 (Did not count the header, 1 line)\n",
        "\n",
        "Top 6 EFFECTs:\n",
        "\n",
        "1. NON_SYNONYMOUS_CODING: 70726 (**Missense mutation**) \n",
        "2. SYNONYMOUS_CODING: 28713 (**Silent mutation**)\n",
        "3. CODON_CHANGE_PLUS_CODON_DELETION: 4118 (Deletion mutation)\n",
        "4. CODON_DELETION: 2724 (?)\n",
        "5. STOP_GAINED: 2241 (**Nonsense mutation**)\n",
        "6. FRAME_SHIFT: 1232\n"
      ]
    }
  ]
}